{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "ir",
      "display_name": "R"
    },
    "language_info": {
      "name": "R"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JaimeLopezGarrido/labo2025v/blob/main/src/ensembles/594_TareaHogar_05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tarea para el Hogar 05"
      ],
      "metadata": {
        "id": "0cEmzeUKFkPh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esta Tarea para el Hogar 05 se entrega el final de la cuarta clase\n",
        "<br> se espera de usted que intente avanzar con los desafios propuestos y que los traiga terminados para la Clase 05 que será el miercoles 03 de septiembre"
      ],
      "metadata": {
        "id": "nSICPpyTGQmC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  1. Overfitting the Public Leaderboard"
      ],
      "metadata": {
        "id": "DenyKXkiJ5JN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Leer  https://medium.com/hmif-itb/overfitting-the-leaderboard-da25172ac62e\n",
        "( 8 minutos )"
      ],
      "metadata": {
        "id": "l-K2_ZsZGrVD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Hiperparámetros del LightGBM"
      ],
      "metadata": {
        "id": "K9GkTOk5J9t3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Los objetivos de esta tarea son:\n",
        "\n",
        "\n",
        "*   Aumentar la rentabilidad de la campaña de marketing de retención proactiva de clientes.\n",
        "*   Generar un mejor modelo optimizando sus hiperparámetros\n",
        "*   Conceptual : investigar los mas relevantes hiperparámetros de LightGBM\n",
        "*   Familiarizarse con la Bayesian Optimization, sus largos tiempos de corrida y opciones para reducirlos\n",
        "*   Familiarizarse con el uso de máquinas virtuales de Google Colab\n",
        "*   Ver un pipeline completo de optimización de hiperparámetros y puesta en producción"
      ],
      "metadata": {
        "id": "VmEFy0ukKL5T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LightGBM cuenta con mas de 60 hiperparámetros, siendo posible utilizar 40 al mismo tiempo, aunque no razonable.\n",
        "<br> La documentación oficial de los hiperparámetros de LightGBM es  https://lightgbm.readthedocs.io/en/latest/Parameters.html#core-parameters\n"
      ],
      "metadata": {
        "id": "5yvlS6JQLRMd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se lo alerta sobre que una Optimizacion Bayesiana lleva varias horas de corrida, y usted deberá correr VARIAS optimizaciones para descubrir cuales parámetros conviene optimizar.\n",
        "<br> A pesar que la próxima clase es recien en viernes 01 de agosto, inicie la tarea con tiempo, aprenda a planificar estratégicamente sus corridas como un@ científ@  de datos."
      ],
      "metadata": {
        "id": "eydI4YNAsFaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Es necesario investigar cuales son los hiperparámetros de LightGBM que vale la pena optimizar en una Bayesian Optimization, ya que los realmente utiles son apenas un reducido subconjunto.\n",
        "<br>Usted deberá investigar cuales son los hiperparámetros mas relevantes de LightGBM, su primer alternativa es preguntándole a su amigo con capacidades especiales ChatGPT o sus endogámicos familiares Claude, DeepSeek, Gemini, Grok, etc\n",
        "<br> La segunda alternativa es la propia documentación de LightGBM  https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html\n"
      ],
      "metadata": {
        "id": "RzU4S0SeMcpp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adicionalmente podra buscar información como la que proveen esta diminuta muestra aleatoria de artículos ligeros:\n",
        "* https://machinelearningmastery.com/light-gradient-boosted-machine-lightgbm-ensemble/\n",
        "*  https://medium.com/@sarahzouinina/a-deep-dive-into-lightgbm-how-to-choose-and-tune-parameters-7c584945842e\n",
        "*  https://www.kaggle.com/code/somang1418/tuning-hyperparameters-under-10-minutes-lgbm\n",
        "*  https://towardsdatascience.com/beginners-guide-to-the-must-know-lightgbm-hyperparameters-a0005a812702/\n",
        "\n",
        "\n",
        "<br>  La muestra anterior se brinda a modo de ejemplo, usted deberá buscar muuuuchas  fuentes adicionales de información\n",
        "<br> Tenga presente que LightGBM es el estado del arte en modelado predictivo para datasets estructurado, que son el 90% del trabajo del 95% de los Data Scientists en Argentina."
      ],
      "metadata": {
        "id": "LNptUgI_NWWG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El desafío de esta tarea es:\n",
        "* Qué hiperparparámetros conviene optimizar?  Las recomendaciones de los artículos ligeros es siempre sensata?  Sus autores realmente hicieron experimentos o son siemplemente escritores de entretenimiento carente de base científica?\n",
        "* Elegidos los hiperparámetros, cual es el  <desde, hasta> que se debe utilizar en la Bayesian Optimization ?\n",
        "* Realmente vale la pena optimizar 10 o 16 hiperparámetros al mismo tiempo ?  No resulta contraproducente una búsqueda en un espacio de tal alta dimensionalidad ?"
      ],
      "metadata": {
        "id": "WpUThBojODyK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "# Resumen Hiperparametros\n",
        "\n",
        "LightGBM utiliza el algoritmo de crecimiento de árbol \"leaf-wise\" (por hoja), mientras que muchas otras herramientas populares utilizan el crecimiento \"depth-wise\" (por profundidad).\n",
        "\n",
        "Parámetros Importantes para el Árbol \"Leaf-wise\":\n",
        "\n",
        "- *num_leaves*  = Número máximo de hojas que cada árbol puede tener; controla complejidad.\n",
        "\n",
        "      Este es el parámetro principal para controlar la complejidad del modelo de árbol. Una profundidad no restringida puede inducir el sobreajuste.$num\\_leaves = 2^{(max\\_depth)}$ para obtener el mismo número de hojas que un árbol \"depth-wise\".\n",
        "\n",
        "      - Tiene que ser menor que 2^{(max\\_depth)} para evitar overfitting.\n",
        "      - Disminuirlo acelera el entrenamiento y reduce overfitting.\n",
        "\n",
        "- *min_data_in_leaf* = Número mínimo de observaciones que cada hoja debe contener.\n",
        "      \n",
        "      Muy importante para prevenir overfitting, ya que previene hojas con muy pocos datos. Cientos o miles es suficiente para un conjunto de datos grande.\n",
        "      - Si sobreajusta hay que subirlo\n",
        "      - Si subajusta hay que bajarlo\n",
        "\n",
        "- *max_depth* = Profundidad máxima de cada árbol; ≤0 significa sin límite. Disminuirlo reduce tiempo y sobreajuste.\n",
        "\n",
        "      -1 → sin límite (LightGBM lo decide por num_leaves)\n",
        "\n",
        "      3–8 → árboles pequeños y conservadores\n",
        "\n",
        "      8–15 → árboles medianos\n",
        "\n",
        "      > 15 → árboles muy complejos, riesgo de sobreajuste\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "# Arboles, menos profundos, mas rapidos, pero menos precisos.\n",
        "\n",
        "\n",
        "- Disminuir *max_depth*: Reducir la profundidad máxima del árbol para limitar el crecimiento y el tiempo de entrenamiento.\n",
        "- Disminuir *num_leaves*: Reducir el número máximo de nodos hoja por árbol, controlando la complejidad y disminuyendo el tiempo de entrenamiento.\n",
        "- Aumentar *min_gain_to_split*: Incrementar la ganancia mínima requerida para una división, evitando divisiones de mejora muy pequeña y acortando el entrenamiento.\n",
        "- Aumentar *min_data_in_leaf* y *min_sum_hessian_in_leaf*: Aumentar estos valores para prevenir la creación de nodos de árbol muy específicos (sobreajuste) y reducir el tiempo de entrenamiento.\n",
        "\n",
        "---\n",
        "\n",
        "# Entrenar menos árboles\n",
        "*num_iterations*\n",
        "\n",
        "- Número de árboles o rondas de boosting.\n",
        "\n",
        "- Valores típicos: 500–2000.\n",
        "\n",
        "- Menos iteraciones → menos tiempo de entrenamiento.\n",
        "\n",
        "- Si se reduce num_iterations, conviene subir learning_rate (por ejemplo, de 0.03 a 0.1).\n",
        "\n",
        "*learning_rate*\n",
        "\n",
        "- Cuánto se ajusta el modelo en cada iteración.\n",
        "\n",
        "- Valores típicos: 0.01–0.1.\n",
        "\n",
        "- No afecta el tiempo de entrenamiento, pero sí la precisión.\n",
        "\n",
        "*early_stopping_round*\n",
        "\n",
        "- Detiene el entrenamiento si no mejora tras X iteraciones.\n",
        "\n",
        "- Ejemplo: early_stopping_round = 50.\n",
        "\n",
        "- Requiere usar un set de validación.\n",
        "\n",
        "- Permite ahorrar tiempo sin perder rendimiento.\n",
        "\n",
        "---\n",
        "\n",
        "# Menos divisiones por nodo\n",
        "*feature_pre_filter*\n",
        "\n",
        "Si está en TRUE (por defecto), LightGBM filtra variables con poca información antes de entrenar. Mantenerlo en TRUE para acelerar.\n",
        "\n",
        "*max_bin*\n",
        "\n",
        "- Cantidad máxima de bins por variable continua.\n",
        "\n",
        "- Menor valor = entrenamiento más rápido pero menos preciso.\n",
        "\n",
        "- Valores típicos: 31–255.\n",
        "\n",
        "- Reducirlo acorta el tiempo de entrenamiento.\n",
        "\n",
        "*min_data_in_bin*\n",
        "\n",
        "- Mínimo de datos por bin.\n",
        "\n",
        "- Aumentarlo reduce el número de bins y acelera el proceso.\n",
        "\n",
        "- Valor típico: 3–10.\n",
        "\n",
        "*feature_fraction*\n",
        "\n",
        "- Porcentaje de variables usadas en cada árbol.\n",
        "\n",
        "- Ejemplo: feature_fraction = 0.5 usa el 50% de las variables al azar.\n",
        "\n",
        "- Reducirlo mejora velocidad y ayuda a evitar sobreajuste.\n",
        "\n",
        "- Valores típicos: 0.5–0.9.\n",
        "\n",
        "*max_cat_threshold*\n",
        "\n",
        "- Número máximo de combinaciones evaluadas para variables categóricas.\n",
        "\n",
        "- Reducirlo disminuye tiempo de cómputo.\n",
        "\n",
        "- Valores comunes: 16–64.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Usar menos datos por iteración\n",
        "*bagging_fraction* y *bagging_freq*\n",
        "\n",
        "Permiten usar una muestra aleatoria del dataset en cada iteración.\n",
        "\n",
        "Ejemplo:\n",
        "\n",
        "- bagging_fraction = 0.75 → usa el 75% de los datos.\n",
        "\n",
        "- bagging_freq = 5 → cambia la muestra cada 5 iteraciones.\n",
        "\n",
        "Reducir bagging_fraction acelera el entrenamiento y puede mejorar generalización.\n",
        "\n",
        "---\n",
        "\n",
        "# Para una Mejor Precisión\n",
        "\n",
        "Para maximizar la precisión del modelo (generalmente a costa de un mayor tiempo de entrenamiento):\n",
        "\n",
        "- Usar un max_bin grande (puede ser más lento).\n",
        "\n",
        "- Usar un learning_rate pequeño con un num_iterations grande.\n",
        "\n",
        "- Usar un num_leaves grande (puede causar sobreajuste).\n",
        "\n",
        "- Usar datos de entrenamiento más grandes.\n",
        "\n",
        "- Probar el booster dart.\n",
        "\n",
        "**Cómo Lidiar con el Sobreajuste**\n",
        "\n",
        "Para ayudar al modelo a generalizar mejor y evitar el sobreajuste:\n",
        "\n",
        "- Usar un max_bin pequeño.\n",
        "\n",
        "- Usar un num_leaves pequeño.\n",
        "\n",
        "- Usar min_data_in_leaf y min_sum_hessian_in_leaf.\n",
        "\n",
        "- Usar bagging (agregación de bootstrap) configurando bagging_fraction y bagging_freq.\n",
        "\n",
        "- Usar submuestreo de características (feature sub-sampling) configurando feature_fraction.\n",
        "\n",
        "- Usar datos de entrenamiento más grandes.\n",
        "\n",
        "- Probar lambda_l1, lambda_l2 y min_gain_to_split para regularización.\n",
        "\n",
        "- Probar max_depth para evitar el crecimiento de árboles profundos.\n",
        "\n",
        "- Probar el algoritmo extra_trees.\n",
        "\n",
        "- Probar aumentar path_smooth."
      ],
      "metadata": {
        "id": "gE9JSs3PdZTD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.1  Seteo del ambiente en Google Colab"
      ],
      "metadata": {
        "id": "PX0qg_c0yqob"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGY7H9xza7Zr"
      },
      "source": [
        "Esta parte se debe correr con el runtime en Python3\n",
        "<br>Ir al menu, Runtime -> Change Runtime Type -> Runtime type ->  **Python 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PupIBNba7Zr"
      },
      "source": [
        "Conectar la virtual machine donde esta corriendo Google Colab con el  Google Drive, para poder tener persistencia de archivos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9LpZCst5a7Zs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37870b5f-b369-4a60-a6a0-e2775bf1ea1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/.drive\n"
          ]
        }
      ],
      "source": [
        "# primero establecer el Runtime de Python 3\n",
        "from google.colab import drive\n",
        "drive.mount('/content/.drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYC_F-wla7Zs"
      },
      "source": [
        "Para correr la siguiente celda es fundamental en Arranque en Frio haber copiado el archivo kaggle.json al Google Drive, en la carpeta indicada en el instructivo\n",
        "\n",
        "<br>los siguientes comando estan en shell script de Linux\n",
        "*   Crear las carpetas en el Google Drive\n",
        "*   \"instalar\" el archivo kaggle.json desde el Google Drive a la virtual machine para que pueda ser utilizado por la libreria  kaggle de Python\n",
        "*   Bajar el  **dataset_pequeno**  al  Google Drive  y tambien al disco local de la virtual machine que esta corriendo Google Colab\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XWLelftXa7Zt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efda8c68-0e9b-4f3b-9765-ac26af6e6df3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "%%shell\n",
        "\n",
        "mkdir -p \"/content/.drive/My Drive/labo1\"\n",
        "mkdir -p \"/content/buckets\"\n",
        "ln -s \"/content/.drive/My Drive/labo1\" /content/buckets/b1\n",
        "\n",
        "mkdir -p ~/.kaggle\n",
        "cp /content/buckets/b1/kaggle/kaggle.json  ~/.kaggle\n",
        "chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "\n",
        "mkdir -p /content/buckets/b1/exp\n",
        "mkdir -p /content/buckets/b1/datasets\n",
        "mkdir -p /content/datasets\n",
        "\n",
        "\n",
        "\n",
        "archivo_origen=\"https://storage.googleapis.com/open-courses/austral2025-af91/dataset_pequeno.csv\"\n",
        "archivo_destino=\"/content/datasets/dataset_pequeno.csv\"\n",
        "archivo_destino_bucket=\"/content/buckets/b1/datasets/dataset_pequeno.csv\"\n",
        "\n",
        "if ! test -f $archivo_destino_bucket; then\n",
        "  wget  $archivo_origen  -O $archivo_destino_bucket\n",
        "fi\n",
        "\n",
        "\n",
        "if ! test -f $archivo_destino; then\n",
        "  cp  $archivo_destino_bucket  $archivo_destino\n",
        "fi\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Optimizacion Hiperparámetros"
      ],
      "metadata": {
        "id": "oSKhZRToy2F7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esta parte se debe correr con el runtime en lenguaje R Ir al menu, Runtime -> Change Runtime Type -> Runtime type -> R"
      ],
      "metadata": {
        "id": "2kwPpHAtSmix"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2.1 Inicio"
      ],
      "metadata": {
        "id": "xp4-Bj3aYI8d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "limpio el ambiente de R"
      ],
      "metadata": {
        "id": "zy8YTZfESxeJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "format(Sys.time(), \"%a %b %d %X %Y\")"
      ],
      "metadata": {
        "id": "gBq__iAdQliq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cbb9a80a-db01-420b-9d69-2daf85d6afa4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "'Sat Nov 01 02:54:55 PM 2025'"
            ],
            "text/markdown": "'Sat Nov 01 02:54:55 PM 2025'",
            "text/latex": "'Sat Nov 01 02:54:55 PM 2025'",
            "text/plain": [
              "[1] \"Sat Nov 01 02:54:55 PM 2025\""
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# limpio la memoria\n",
        "rm(list=ls(all.names=TRUE)) # remove all objects\n",
        "gc(full=TRUE, verbose=FALSE) # garbage collection"
      ],
      "metadata": {
        "id": "7rdVrBojS1IV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "1cbbbb8d-84d3-4290-97e1-ace579b60d43"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table class=\"dataframe\">\n",
              "<caption>A matrix: 2 × 6 of type dbl</caption>\n",
              "<thead>\n",
              "\t<tr><th></th><th scope=col>used</th><th scope=col>(Mb)</th><th scope=col>gc trigger</th><th scope=col>(Mb)</th><th scope=col>max used</th><th scope=col>(Mb)</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "\t<tr><th scope=row>Ncells</th><td> 660381</td><td>35.3</td><td>1454462</td><td>77.7</td><td>1454462</td><td>77.7</td></tr>\n",
              "\t<tr><th scope=row>Vcells</th><td>1226627</td><td> 9.4</td><td>8388608</td><td>64.0</td><td>1975128</td><td>15.1</td></tr>\n",
              "</tbody>\n",
              "</table>\n"
            ],
            "text/markdown": "\nA matrix: 2 × 6 of type dbl\n\n| <!--/--> | used | (Mb) | gc trigger | (Mb) | max used | (Mb) |\n|---|---|---|---|---|---|---|\n| Ncells |  660381 | 35.3 | 1454462 | 77.7 | 1454462 | 77.7 |\n| Vcells | 1226627 |  9.4 | 8388608 | 64.0 | 1975128 | 15.1 |\n\n",
            "text/latex": "A matrix: 2 × 6 of type dbl\n\\begin{tabular}{r|llllll}\n  & used & (Mb) & gc trigger & (Mb) & max used & (Mb)\\\\\n\\hline\n\tNcells &  660381 & 35.3 & 1454462 & 77.7 & 1454462 & 77.7\\\\\n\tVcells & 1226627 &  9.4 & 8388608 & 64.0 & 1975128 & 15.1\\\\\n\\end{tabular}\n",
            "text/plain": [
              "       used    (Mb) gc trigger (Mb) max used (Mb)\n",
              "Ncells  660381 35.3 1454462    77.7 1454462  77.7\n",
              "Vcells 1226627  9.4 8388608    64.0 1975128  15.1"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2.2 Carga de Librerias"
      ],
      "metadata": {
        "id": "kuPfQ7ksjwW3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cargo las librerias que necesito\n",
        "require(\"data.table\")\n",
        "require(\"parallel\")\n",
        "\n",
        "if( !require(\"primes\") ) install.packages(\"primes\")\n",
        "require(\"primes\")\n",
        "\n",
        "if( !require(\"utils\") ) install.packages(\"utils\")\n",
        "require(\"utils\")\n",
        "\n",
        "if( !require(\"rlist\") ) install.packages(\"rlist\")\n",
        "require(\"rlist\")\n",
        "\n",
        "if( !require(\"yaml\")) install.packages(\"yaml\")\n",
        "require(\"yaml\")\n",
        "\n",
        "if( !require(\"lightgbm\") ) install.packages(\"lightgbm\")\n",
        "require(\"lightgbm\")\n",
        "\n",
        "if( !require(\"DiceKriging\") ) install.packages(\"DiceKriging\")\n",
        "require(\"DiceKriging\")\n",
        "\n",
        "if( !require(\"mlrMBO\") ) install.packages(\"mlrMBO\")\n",
        "require(\"mlrMBO\")"
      ],
      "metadata": {
        "id": "lVyxLaJ1j1J_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75c0b8d7-11d7-4a13-cad9-b30add06eeaa"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading required package: data.table\n",
            "\n",
            "Loading required package: parallel\n",
            "\n",
            "Loading required package: primes\n",
            "\n",
            "Warning message in library(package, lib.loc = lib.loc, character.only = TRUE, logical.return = TRUE, :\n",
            "“there is no package called ‘primes’”\n",
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "Loading required package: primes\n",
            "\n",
            "Loading required package: rlist\n",
            "\n",
            "Warning message in library(package, lib.loc = lib.loc, character.only = TRUE, logical.return = TRUE, :\n",
            "“there is no package called ‘rlist’”\n",
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "also installing the dependency ‘XML’\n",
            "\n",
            "\n",
            "Loading required package: rlist\n",
            "\n",
            "Loading required package: yaml\n",
            "\n",
            "Loading required package: lightgbm\n",
            "\n",
            "Warning message in library(package, lib.loc = lib.loc, character.only = TRUE, logical.return = TRUE, :\n",
            "“there is no package called ‘lightgbm’”\n",
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "Loading required package: lightgbm\n",
            "\n",
            "Loading required package: DiceKriging\n",
            "\n",
            "Warning message in library(package, lib.loc = lib.loc, character.only = TRUE, logical.return = TRUE, :\n",
            "“there is no package called ‘DiceKriging’”\n",
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "Loading required package: DiceKriging\n",
            "\n",
            "Loading required package: mlrMBO\n",
            "\n",
            "Warning message in library(package, lib.loc = lib.loc, character.only = TRUE, logical.return = TRUE, :\n",
            "“there is no package called ‘mlrMBO’”\n",
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "also installing the dependencies ‘fastmatch’, ‘RcppArmadillo’, ‘mlr’, ‘ParamHelpers’, ‘smoof’, ‘BBmisc’, ‘checkmate’, ‘lhs’, ‘parallelMap’\n",
            "\n",
            "\n",
            "Loading required package: mlrMBO\n",
            "\n",
            "Loading required package: mlr\n",
            "\n",
            "Loading required package: ParamHelpers\n",
            "\n",
            "Loading required package: smoof\n",
            "\n",
            "Loading required package: checkmate\n",
            "\n",
            "\n",
            "Attaching package: ‘checkmate’\n",
            "\n",
            "\n",
            "The following object is masked from ‘package:DiceKriging’:\n",
            "\n",
            "    checkNames\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2.3 Definicion de Parametros"
      ],
      "metadata": {
        "id": "Iz-6Qt6BUaA3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "aqui debe cargar SU semilla primigenia\n",
        "<br>recuerde cambiar el numero de experimento en cada corrida nueva"
      ],
      "metadata": {
        "id": "cOdlKd7lUm2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PARAM <- list()\n",
        "PARAM$experimento <- 6001\n",
        "PARAM$semilla_primigenia <- 100007\n"
      ],
      "metadata": {
        "id": "ASYkebOu2mF6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PARAM$kaggle$competencia <- \"labo-i-2025-virtual-analista-sr\"\n",
        "PARAM$kaggle$cortes <- seq(10000, 12000, by= 500)"
      ],
      "metadata": {
        "id": "ezOhQdbA293o"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# un undersampling de 0.1  toma solo el 10% de los CONTINUA\n",
        "# undersampling de 1.0  implica tomar TODOS los datos\n",
        "\n",
        "PARAM$trainingstrategy$undersampling <- 0.5"
      ],
      "metadata": {
        "id": "jtB0Lub42rHO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parametros LightGBM\n",
        "\n",
        "PARAM$hyperparametertuning$xval_folds <- 5\n",
        "\n",
        "# parametros fijos del LightGBM que se pisaran con la parte variable de la BO\n",
        "PARAM$lgbm$param_fijos <-  list(\n",
        "  boosting= \"gbdt\", # puede ir  dart  , ni pruebe random_forest\n",
        "  objective= \"binary\",\n",
        "  metric= \"auc\",\n",
        "  first_metric_only= FALSE,\n",
        "  boost_from_average= TRUE,\n",
        "  feature_pre_filter= FALSE,\n",
        "  force_row_wise= TRUE, # para reducir warnings\n",
        "  verbosity= -100,\n",
        "\n",
        "  seed= PARAM$semilla_primigenia,\n",
        "\n",
        "  max_depth= -1L, # -1 significa no limitar,  por ahora lo dejo fijo\n",
        "  min_gain_to_split= 0, # min_gain_to_split >= 0\n",
        "  min_sum_hessian_in_leaf= 0.001, #  min_sum_hessian_in_leaf >= 0.0\n",
        "  lambda_l1= 0.0, # lambda_l1 >= 0.0\n",
        "  lambda_l2= 0.0, # lambda_l2 >= 0.0\n",
        "  max_bin= 31L, # lo debo dejar fijo, no participa de la BO\n",
        "\n",
        "  bagging_fraction= 1.0, # 0.0 < bagging_fraction <= 1.0\n",
        "  pos_bagging_fraction= 1.0, # 0.0 < pos_bagging_fraction <= 1.0\n",
        "  neg_bagging_fraction= 1.0, # 0.0 < neg_bagging_fraction <= 1.0\n",
        "  is_unbalance= FALSE, #\n",
        "  scale_pos_weight= 1.0, # scale_pos_weight > 0.0\n",
        "\n",
        "  drop_rate= 0.1, # 0.0 < neg_bagging_fraction <= 1.0\n",
        "  max_drop= 50, # <=0 means no limit\n",
        "  skip_drop= 0.5, # 0.0 <= skip_drop <= 1.0\n",
        "\n",
        "  extra_trees= FALSE,\n",
        "\n",
        "  num_iterations= 1200,\n",
        "  learning_rate= 0.02,\n",
        "  feature_fraction= 0.5,\n",
        "  num_leaves= 750,\n",
        "  min_data_in_leaf= 5000\n",
        ")\n"
      ],
      "metadata": {
        "id": "OFxm-xiNUOJX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui se definen los hiperparámetros de LightGBM que participan de la Bayesian Optimization\n",
        "<br> si es un numero entero debe ir  makeIntegerParam\n",
        "<br> si es un numero real (con decimales) debe ir  makeNumericParam\n",
        "<br> es muy importante leer cuales son un lower y upper  permitidos y ademas razonables"
      ],
      "metadata": {
        "id": "D5Yj-JV4yvOt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Aqui se cargan los bordes de los hiperparametros de la BO\n",
        "#PARAM$hypeparametertuning$hs <- makeParamSet(\n",
        "#  makeNumericParam(\"learning_rate\", lower = 0.01, upper = 0.05),\n",
        "#  makeIntegerParam(\"num_leaves\", lower = 16L, upper = 128L),\n",
        "#  makeNumericParam(\"feature_fraction\", lower = 0.5, upper = 0.9),\n",
        "#  makeNumericParam(\"bagging_fraction\", lower = 0.7, upper = 1.0),\n",
        "#  makeIntegerParam(\"min_data_in_leaf\", lower = 20L, upper = 200L),\n",
        "#  makeNumericParam(\"lambda_l1\", lower = 0, upper = 10),\n",
        "#  makeNumericParam(\"lambda_l2\", lower = 0, upper = 10),\n",
        "#  makeNumericParam(\"min_gain_to_split\", lower = 0, upper = 0.5),\n",
        "#  makeIntegerParam(\"max_depth\", lower = -1L, upper = 10L)\n",
        "#)\n",
        "\n",
        "PARAM$hypeparametertuning$hs <- makeParamSet(\n",
        "  makeNumericParam(\"learning_rate\", lower = 0.01, upper = 0.03),\n",
        "  makeIntegerParam(\"num_leaves\", lower = 32L, upper = 96L),\n",
        "  makeNumericParam(\"feature_fraction\", lower = 0.6, upper = 0.9),\n",
        "  makeNumericParam(\"bagging_fraction\", lower = 0.7, upper = 1.0),\n",
        "  makeIntegerParam(\"bagging_freq\", lower = 0L, upper = 10L),\n",
        "  makeIntegerParam(\"min_data_in_leaf\", lower = 50L, upper = 250L),\n",
        "  makeNumericParam(\"lambda_l1\", lower = 0, upper = 10),\n",
        "  makeNumericParam(\"lambda_l2\", lower = 0, upper = 10),\n",
        "  makeNumericParam(\"min_gain_to_split\", lower = 0, upper = 0.3),\n",
        "  makeIntegerParam(\"max_depth\", lower = -1L, upper = 10L),\n",
        "  makeNumericParam(\"min_sum_hessian_in_leaf\", lower = 0.0001, upper = 10),\n",
        "  makeNumericParam(\"scale_pos_weight\", lower = 10, upper = 200)\n",
        ")"
      ],
      "metadata": {
        "id": "jENpR26ZyuS8"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A mayor cantidad de hiperparámetros, se debe aumentar las iteraciones de la Bayesian Optimization\n",
        "<br> 30 es un valor muy tacaño, pero corre rápido\n",
        "<br> deberia partir de 50, alcanzando los 100 si se dispone de tiempo"
      ],
      "metadata": {
        "id": "-_RPFUb3zMoW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PARAM$hyperparametertuning$iteraciones <- 35 # iteraciones bayesianas"
      ],
      "metadata": {
        "id": "q5Rd3pnbzSiG"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2.4  Preprocesamiento"
      ],
      "metadata": {
        "id": "4RWZXL1VZjMI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# carpeta de trabajo\n",
        "\n",
        "setwd(\"/content/buckets/b1/exp\")\n",
        "experimento_folder <- paste0(\"HT\", PARAM$experimento)\n",
        "dir.create(experimento_folder, showWarnings=FALSE)\n",
        "setwd( paste0(\"/content/buckets/b1/exp/\", experimento_folder ))"
      ],
      "metadata": {
        "id": "j3toG9-lZm4K"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lectura del dataset\n",
        "\n",
        "dataset <- fread(\"/content/datasets/dataset_pequeno.csv\")"
      ],
      "metadata": {
        "id": "FM3lxKoLZ643"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train <- dataset[foto_mes %in% c(202107)]"
      ],
      "metadata": {
        "id": "OsJ-91UeZ-I_"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# paso la clase a binaria que tome valores {0,1}  enteros\n",
        "#  BAJA+1 y BAJA+2  son  1,   CONTINUA es 0\n",
        "\n",
        "dataset_train[,\n",
        "  clase01 := ifelse(clase_ternaria %in% c(\"BAJA+2\",\"BAJA+1\"), 1L, 0L)\n",
        "]"
      ],
      "metadata": {
        "id": "vrWE7BE0aB2J"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defino los datos que forma parte del training\n",
        "# aqui se hace el undersampling de los CONTINUA\n",
        "# notar que para esto utilizo la SEGUNDA semilla\n",
        "\n",
        "set.seed(PARAM$semilla_primigenia, kind = \"L'Ecuyer-CMRG\")\n",
        "dataset_train[, azar := runif(nrow(dataset_train))]\n",
        "dataset_train[, training := 0L]\n",
        "\n",
        "dataset_train[\n",
        "  foto_mes %in% c(202107) &\n",
        "    (azar <= PARAM$trainingstrategy$undersampling | clase_ternaria %in% c(\"BAJA+1\", \"BAJA+2\")),\n",
        "  training := 1L\n",
        "]"
      ],
      "metadata": {
        "id": "jP7YlQBnaW6W"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# los campos que se van a utilizar\n",
        "\n",
        "campos_buenos <- setdiff(\n",
        "  colnames(dataset_train),\n",
        "  c(\"clase_ternaria\", \"clase01\", \"azar\", \"training\")\n",
        ")"
      ],
      "metadata": {
        "id": "xElu4s5W4rX7"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dejo los datos en el formato que necesita LightGBM\n",
        "\n",
        "dtrain <- lgb.Dataset(\n",
        "  data= data.matrix(dataset_train[training == 1L, campos_buenos, with= FALSE]),\n",
        "  label= dataset_train[training == 1L, clase01],\n",
        "  free_raw_data= FALSE\n",
        ")\n",
        "\n",
        "nrow(dtrain)\n",
        "ncol(dtrain)"
      ],
      "metadata": {
        "id": "PppMHcGYaaol",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "93e8873d-58d9-4f63-e18f-519ae2c367b6"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "83029"
            ],
            "text/markdown": "83029",
            "text/latex": "83029",
            "text/plain": [
              "[1] 83029"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "154"
            ],
            "text/markdown": "154",
            "text/latex": "154",
            "text/plain": [
              "[1] 154"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.2.5 Configuracion Bayesian Optimization"
      ],
      "metadata": {
        "id": "Ta-EkOu3cphF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# En el argumento x llegan los parmaetros de la bayesiana\n",
        "#  devuelve la AUC en cross validation del modelo entrenado\n",
        "\n",
        "EstimarGanancia_AUC_lightgbm <- function(x) {\n",
        "\n",
        "  # x pisa (o agrega) a param_fijos\n",
        "  param_completo <- modifyList(PARAM$lgbm$param_fijos, x)\n",
        "\n",
        "  # entreno LightGBM\n",
        "  modelocv <- lgb.cv(\n",
        "    data= dtrain,\n",
        "    nfold= PARAM$hyperparametertuning$xval_folds,\n",
        "    stratified= TRUE,\n",
        "    param= as.list(param_completo[names(param_completo) != \"boost_from_average\"]) # Ensure param is a named list/vector\n",
        "  )\n",
        "\n",
        "  # obtengo la ganancia\n",
        "  AUC <- modelocv$best_score\n",
        "\n",
        "  # hago espacio en la memoria\n",
        "  rm(modelocv)\n",
        "  gc(full= TRUE, verbose= FALSE)\n",
        "\n",
        "  message(format(Sys.time(), \"%a %b %d %X %Y\"), \" AUC \", AUC)\n",
        "\n",
        "  return(AUC)\n",
        "}"
      ],
      "metadata": {
        "id": "cjgfurjdfiXb"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aqui comienza la configuracion de la Bayesian Optimization\n",
        "\n",
        "# en este archivo quedan la evolucion binaria de la BO\n",
        "kbayesiana <- \"bayesiana.RDATA\"\n",
        "\n",
        "funcion_optimizar <- EstimarGanancia_AUC_lightgbm # la funcion que voy a maximizar\n",
        "\n",
        "configureMlr(show.learner.output= FALSE)\n",
        "\n",
        "# configuro la busqueda bayesiana,  los hiperparametros que se van a optimizar\n",
        "# por favor, no desesperarse por lo complejo\n",
        "\n",
        "obj.fun <- makeSingleObjectiveFunction(\n",
        "  fn= funcion_optimizar, # la funcion que voy a maximizar\n",
        "  minimize= FALSE, # estoy Maximizando la ganancia\n",
        "  noisy= TRUE,\n",
        "  par.set= PARAM$hypeparametertuning$hs, # definido al comienzo del programa\n",
        "  has.simple.signature= FALSE # paso los parametros en una lista\n",
        ")\n",
        "\n",
        "# cada 600 segundos guardo el resultado intermedio\n",
        "ctrl <- makeMBOControl(\n",
        "  save.on.disk.at.time= 600, # se graba cada 600 segundos\n",
        "  save.file.path= kbayesiana\n",
        ") # se graba cada 600 segundos\n",
        "\n",
        "# indico la cantidad de iteraciones que va a tener la Bayesian Optimization\n",
        "ctrl <- setMBOControlTermination(\n",
        "  ctrl,\n",
        "  iters= PARAM$hyperparametertuning$iteraciones\n",
        ") # cantidad de iteraciones\n",
        "\n",
        "# defino el método estandar para la creacion de los puntos iniciales,\n",
        "# los \"No Inteligentes\"\n",
        "ctrl <- setMBOControlInfill(ctrl, crit= makeMBOInfillCritEI())\n",
        "\n",
        "# establezco la funcion que busca el maximo\n",
        "surr.km <- makeLearner(\n",
        "  \"regr.km\",\n",
        "  predict.type= \"se\",\n",
        "  covtype= \"matern3_2\",\n",
        "  control= list(trace= TRUE)\n",
        ")\n"
      ],
      "metadata": {
        "id": "WLi_o1hocvN-"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.2.6 Corrida Bayesian Optimization"
      ],
      "metadata": {
        "id": "_uUeVo5pc4zc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# inicio la optimizacion bayesiana, retomando si ya existe\n",
        "# es la celda mas lenta de todo el notebook\n",
        "\n",
        "if (!file.exists(kbayesiana)) {\n",
        "  bayesiana_salida <- mbo(obj.fun, learner= surr.km, control= ctrl)\n",
        "} else {\n",
        "  bayesiana_salida <- mboContinue(kbayesiana) # retomo en caso que ya exista\n",
        "}"
      ],
      "metadata": {
        "id": "RcABNaKGciaz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7e2bb34-3cde-4998-f9c2-23894d622b77"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Computing y column(s) for design. Not provided.\n",
            "\n",
            "Sat Nov 01 03:20:36 PM 2025 AUC 0.924989975139509\n",
            "\n",
            "Sat Nov 01 03:24:13 PM 2025 AUC 0.925875229333851\n",
            "\n",
            "Sat Nov 01 03:26:14 PM 2025 AUC 0.925724504910479\n",
            "\n",
            "Sat Nov 01 03:30:10 PM 2025 AUC 0.926992230818329\n",
            "\n",
            "Sat Nov 01 03:32:05 PM 2025 AUC 0.919204477518979\n",
            "\n",
            "Sat Nov 01 03:34:22 PM 2025 AUC 0.925409655609331\n",
            "\n",
            "Sat Nov 01 03:37:22 PM 2025 AUC 0.925791252896511\n",
            "\n",
            "Sat Nov 01 03:41:27 PM 2025 AUC 0.923927993140774\n",
            "\n",
            "Sat Nov 01 03:44:14 PM 2025 AUC 0.927982869044679\n",
            "\n",
            "Sat Nov 01 03:47:34 PM 2025 AUC 0.924411383120829\n",
            "\n",
            "Sat Nov 01 03:52:46 PM 2025 AUC 0.928768668053785\n",
            "\n",
            "Sat Nov 01 03:56:08 PM 2025 AUC 0.927440662340898\n",
            "\n",
            "Sat Nov 01 03:59:00 PM 2025 AUC 0.928150015425845\n",
            "\n",
            "Sat Nov 01 04:02:17 PM 2025 AUC 0.927771790959397\n",
            "\n",
            "Sat Nov 01 04:04:51 PM 2025 AUC 0.9254416719633\n",
            "\n",
            "Sat Nov 01 04:06:50 PM 2025 AUC 0.927025869579548\n",
            "\n",
            "Sat Nov 01 04:11:26 PM 2025 AUC 0.927609620210988\n",
            "\n",
            "Sat Nov 01 04:13:35 PM 2025 AUC 0.924462095197052\n",
            "\n",
            "Sat Nov 01 04:16:07 PM 2025 AUC 0.926085659729748\n",
            "\n",
            "Sat Nov 01 04:19:17 PM 2025 AUC 0.928406232840202\n",
            "\n",
            "Sat Nov 01 04:20:29 PM 2025 AUC 0.919111349923759\n",
            "\n",
            "Sat Nov 01 04:24:26 PM 2025 AUC 0.926328251972529\n",
            "\n",
            "Sat Nov 01 04:27:51 PM 2025 AUC 0.924835441138483\n",
            "\n",
            "Sat Nov 01 04:30:06 PM 2025 AUC 0.921007105176949\n",
            "\n",
            "Sat Nov 01 04:33:40 PM 2025 AUC 0.929446613826166\n",
            "\n",
            "Sat Nov 01 04:39:40 PM 2025 AUC 0.929697873663059\n",
            "\n",
            "Sat Nov 01 04:42:49 PM 2025 AUC 0.925251621070929\n",
            "\n",
            "Sat Nov 01 04:44:38 PM 2025 AUC 0.92563298028921\n",
            "\n",
            "Sat Nov 01 04:49:18 PM 2025 AUC 0.925975120830712\n",
            "\n",
            "Sat Nov 01 04:53:53 PM 2025 AUC 0.928525112929726\n",
            "\n",
            "Sat Nov 01 04:55:40 PM 2025 AUC 0.922640695087691\n",
            "\n",
            "Sat Nov 01 05:00:36 PM 2025 AUC 0.928189844258428\n",
            "\n",
            "Sat Nov 01 05:06:27 PM 2025 AUC 0.924247528408967\n",
            "\n",
            "Sat Nov 01 05:10:18 PM 2025 AUC 0.927169201474149\n",
            "\n",
            "Sat Nov 01 05:14:01 PM 2025 AUC 0.927337921109444\n",
            "\n",
            "Sat Nov 01 05:17:36 PM 2025 AUC 0.929065119571914\n",
            "\n",
            "Sat Nov 01 05:19:53 PM 2025 AUC 0.91789872176011\n",
            "\n",
            "Sat Nov 01 05:22:02 PM 2025 AUC 0.924761068151929\n",
            "\n",
            "Sat Nov 01 05:26:11 PM 2025 AUC 0.927637101992461\n",
            "\n",
            "Sat Nov 01 05:29:17 PM 2025 AUC 0.924541200870515\n",
            "\n",
            "Sat Nov 01 05:32:02 PM 2025 AUC 0.926701275315622\n",
            "\n",
            "Sat Nov 01 05:35:52 PM 2025 AUC 0.924215591703762\n",
            "\n",
            "Sat Nov 01 05:40:38 PM 2025 AUC 0.928721818272771\n",
            "\n",
            "Sat Nov 01 05:43:06 PM 2025 AUC 0.923001415064854\n",
            "\n",
            "Sat Nov 01 05:46:47 PM 2025 AUC 0.926737480529874\n",
            "\n",
            "Sat Nov 01 05:50:56 PM 2025 AUC 0.925423431639831\n",
            "\n",
            "Sat Nov 01 05:55:17 PM 2025 AUC 0.928554816696598\n",
            "\n",
            "Sat Nov 01 05:57:12 PM 2025 AUC 0.925609919301373\n",
            "\n",
            "[mbo] 0: learning_rate=0.0157; num_leaves=50; feature_fraction=0.676; bagging_fraction=0.773; bagging_freq=8; min_data_in_leaf=106; lambda_l1=5.09; lambda_l2=3.12; min_gain_to_split=0.0815; max_depth=3; min_sum_hessian_in_leaf=6.67; scale_pos_weight=105 : y = 0.925 : 122.0 secs : initdesign\n",
            "\n",
            "[mbo] 0: learning_rate=0.0188; num_leaves=56; feature_fraction=0.806; bagging_fraction=0.875; bagging_freq=8; min_data_in_leaf=167; lambda_l1=5.3; lambda_l2=7.98; min_gain_to_split=0.23; max_depth=7; min_sum_hessian_in_leaf=4.12; scale_pos_weight=105 : y = 0.926 : 216.4 secs : initdesign\n",
            "\n",
            "[mbo] 0: learning_rate=0.0226; num_leaves=53; feature_fraction=0.754; bagging_fraction=0.975; bagging_freq=6; min_data_in_leaf=214; lambda_l1=5.67; lambda_l2=5.87; min_gain_to_split=0.16; max_depth=2; min_sum_hessian_in_leaf=5.83; scale_pos_weight=62.7 : y = 0.926 : 121.9 secs : initdesign\n",
            "\n",
            "[mbo] 0: learning_rate=0.0196; num_leaves=75; feature_fraction=0.793; bagging_fraction=0.826; bagging_freq=4; min_data_in_leaf=232; lambda_l1=7.01; lambda_l2=8.95; min_gain_to_split=0.209; max_depth=7; min_sum_hessian_in_leaf=3.87; scale_pos_weight=156 : y = 0.927 : 235.5 secs : initdesign\n",
            "\n",
            "[mbo] 0: learning_rate=0.0161; num_leaves=83; feature_fraction=0.714; bagging_fraction=0.734; bagging_freq=3; min_data_in_leaf=154; lambda_l1=4.58; lambda_l2=1.04; min_gain_to_split=0.184; max_depth=1; min_sum_hessian_in_leaf=5.39; scale_pos_weight=89.4 : y = 0.919 : 115.3 secs : initdesign\n",
            "\n",
            "[mbo] 0: learning_rate=0.0169; num_leaves=61; feature_fraction=0.615; bagging_fraction=0.784; bagging_freq=6; min_data_in_leaf=157; lambda_l1=1.7; lambda_l2=2.09; min_gain_to_split=0.265; max_depth=2; min_sum_hessian_in_leaf=0.793; scale_pos_weight=59.9 : y = 0.925 : 136.8 secs : initdesign\n",
            "\n",
            "[mbo] 0: learning_rate=0.012; num_leaves=69; feature_fraction=0.824; bagging_fraction=0.96; bagging_freq=7; min_data_in_leaf=191; lambda_l1=2.32; lambda_l2=0.324; min_gain_to_split=0.0748; max_depth=5; min_sum_hessian_in_leaf=2.89; scale_pos_weight=100 : y = 0.926 : 179.8 secs : initdesign\n",
            "\n",
            "[mbo] 0: learning_rate=0.0215; num_leaves=83; feature_fraction=0.811; bagging_fraction=0.755; bagging_freq=9; min_data_in_leaf=173; lambda_l1=3.57; lambda_l2=1.86; min_gain_to_split=0.181; max_depth=9; min_sum_hessian_in_leaf=3.46; scale_pos_weight=151 : y = 0.924 : 245.3 secs : initdesign\n",
            "\n",
            "[mbo] 0: learning_rate=0.0113; num_leaves=70; feature_fraction=0.701; bagging_fraction=0.865; bagging_freq=8; min_data_in_leaf=121; lambda_l1=3.88; lambda_l2=5.57; min_gain_to_split=0.101; max_depth=5; min_sum_hessian_in_leaf=4.73; scale_pos_weight=36 : y = 0.928 : 166.8 secs : initdesign\n",
            "\n",
            "[mbo] 0: learning_rate=0.022; num_leaves=88; feature_fraction=0.892; bagging_fraction=0.936; bagging_freq=2; min_data_in_leaf=100; lambda_l1=1.94; lambda_l2=6.05; min_gain_to_split=0.0438; max_depth=3; min_sum_hessian_in_leaf=6.44; scale_pos_weight=157 : y = 0.924 : 199.9 secs : initdesign\n",
            "\n",
            "[mbo] 0: learning_rate=0.0207; num_leaves=58; feature_fraction=0.643; bagging_fraction=0.922; bagging_freq=3; min_data_in_leaf=85; lambda_l1=6.04; lambda_l2=3.53; min_gain_to_split=0.249; max_depth=-1; min_sum_hessian_in_leaf=4.38; scale_pos_weight=177 : y = 0.929 : 312.2 secs : initdesign\n",
            "\n",
            "[mbo] 0: learning_rate=0.0267; num_leaves=96; feature_fraction=0.782; bagging_fraction=0.805; bagging_freq=0; min_data_in_leaf=79; lambda_l1=9.01; lambda_l2=9.13; min_gain_to_split=0.174; max_depth=0; min_sum_hessian_in_leaf=2.92; scale_pos_weight=119 : y = 0.927 : 202.3 secs : initdesign\n",
            "\n",
            "[mbo] 0: learning_rate=0.0183; num_leaves=47; feature_fraction=0.724; bagging_fraction=0.926; bagging_freq=1; min_data_in_leaf=141; lambda_l1=9.94; lambda_l2=6.82; min_gain_to_split=0.135; max_depth=7; min_sum_hessian_in_leaf=8.65; scale_pos_weight=53 : y = 0.928 : 171.2 secs : initdesign\n",
            "\n",
            "[mbo] 0: learning_rate=0.0172; num_leaves=61; feature_fraction=0.881; bagging_fraction=0.846; bagging_freq=6; min_data_in_leaf=235; lambda_l1=6.4; lambda_l2=1.91; min_gain_to_split=0.271; max_depth=6; min_sum_hessian_in_leaf=6.91; scale_pos_weight=83.8 : y = 0.928 : 197.0 secs : initdesign\n",
            "\n",
            "[mbo] 0: learning_rate=0.0241; num_leaves=65; feature_fraction=0.658; bagging_fraction=0.888; bagging_freq=4; min_data_in_leaf=211; lambda_l1=4.86; lambda_l2=0.444; min_gain_to_split=0.123; max_depth=3; min_sum_hessian_in_leaf=8.46; scale_pos_weight=124 : y = 0.925 : 154.3 secs : initdesign\n",
            "\n",
            "[mbo] 0: learning_rate=0.0252; num_leaves=45; feature_fraction=0.708; bagging_fraction=0.806; bagging_freq=9; min_data_in_leaf=136; lambda_l1=6.11; lambda_l2=6.98; min_gain_to_split=0.0405; max_depth=3; min_sum_hessian_in_leaf=5.74; scale_pos_weight=77.6 : y = 0.927 : 119.1 secs : initdesign\n",
            "\n",
            "[mbo] 0: learning_rate=0.0255; num_leaves=65; feature_fraction=0.646; bagging_fraction=0.969; bagging_freq=7; min_data_in_leaf=164; lambda_l1=9.66; lambda_l2=7.74; min_gain_to_split=0.0269; max_depth=9; min_sum_hessian_in_leaf=8.89; scale_pos_weight=163 : y = 0.928 : 276.5 secs : initdesign\n",
            "\n",
            "[mbo] 0: learning_rate=0.0193; num_leaves=73; feature_fraction=0.731; bagging_fraction=0.715; bagging_freq=9; min_data_in_leaf=72; lambda_l1=2.1; lambda_l2=3.9; min_gain_to_split=0.26; max_depth=4; min_sum_hessian_in_leaf=7.89; scale_pos_weight=136 : y = 0.924 : 128.1 secs : initdesign\n",
            "\n",
            "[mbo] 0: learning_rate=0.0299; num_leaves=87; feature_fraction=0.669; bagging_fraction=0.766; bagging_freq=5; min_data_in_leaf=126; lambda_l1=8.57; lambda_l2=4.23; min_gain_to_split=0.2; max_depth=4; min_sum_hessian_in_leaf=7.53; scale_pos_weight=128 : y = 0.926 : 152.0 secs : initdesign\n",
            "\n",
            "[mbo] 0: learning_rate=0.0278; num_leaves=46; feature_fraction=0.846; bagging_fraction=0.706; bagging_freq=2; min_data_in_leaf=218; lambda_l1=8.15; lambda_l2=5.37; min_gain_to_split=0.215; max_depth=4; min_sum_hessian_in_leaf=5.51; scale_pos_weight=31.1 : y = 0.928 : 190.9 secs : initdesign\n",
            "\n",
            "[mbo] 0: learning_rate=0.0148; num_leaves=38; feature_fraction=0.762; bagging_fraction=0.882; bagging_freq=1; min_data_in_leaf=204; lambda_l1=4.75; lambda_l2=7.09; min_gain_to_split=0.195; max_depth=1; min_sum_hessian_in_leaf=1.27; scale_pos_weight=19.3 : y = 0.919 : 71.3 secs : initdesign\n",
            "\n",
            "[mbo] 0: learning_rate=0.0163; num_leaves=39; feature_fraction=0.764; bagging_fraction=0.989; bagging_freq=10; min_data_in_leaf=120; lambda_l1=3.49; lambda_l2=8.73; min_gain_to_split=0.221; max_depth=10; min_sum_hessian_in_leaf=2.04; scale_pos_weight=114 : y = 0.926 : 237.4 secs : initdesign\n",
            "\n",
            "[mbo] 0: learning_rate=0.0133; num_leaves=77; feature_fraction=0.623; bagging_fraction=0.709; bagging_freq=5; min_data_in_leaf=229; lambda_l1=4.24; lambda_l2=6.47; min_gain_to_split=0.0675; max_depth=6; min_sum_hessian_in_leaf=5.09; scale_pos_weight=186 : y = 0.925 : 205.3 secs : initdesign\n",
            "\n",
            "[mbo] 0: learning_rate=0.0281; num_leaves=93; feature_fraction=0.655; bagging_fraction=0.856; bagging_freq=3; min_data_in_leaf=186; lambda_l1=2.78; lambda_l2=4.45; min_gain_to_split=0.0231; max_depth=1; min_sum_hessian_in_leaf=3.14; scale_pos_weight=132 : y = 0.921 : 134.6 secs : initdesign\n",
            "\n",
            "[mbo] 0: learning_rate=0.0145; num_leaves=59; feature_fraction=0.694; bagging_fraction=0.822; bagging_freq=0; min_data_in_leaf=70; lambda_l1=7.72; lambda_l2=8.15; min_gain_to_split=0.108; max_depth=0; min_sum_hessian_in_leaf=7.43; scale_pos_weight=70.1 : y = 0.929 : 213.8 secs : initdesign\n",
            "\n",
            "[mbo] 0: learning_rate=0.0125; num_leaves=85; feature_fraction=0.861; bagging_fraction=0.878; bagging_freq=3; min_data_in_leaf=193; lambda_l1=5.43; lambda_l2=7.59; min_gain_to_split=0.0975; max_depth=-1; min_sum_hessian_in_leaf=4.95; scale_pos_weight=53.9 : y = 0.93 : 360.2 secs : initdesign\n",
            "\n",
            "[mbo] 0: learning_rate=0.0249; num_leaves=42; feature_fraction=0.632; bagging_fraction=0.983; bagging_freq=0; min_data_in_leaf=116; lambda_l1=7.69; lambda_l2=2.63; min_gain_to_split=0.244; max_depth=8; min_sum_hessian_in_leaf=1.2; scale_pos_weight=87.7 : y = 0.925 : 189.1 secs : initdesign\n",
            "\n",
            "[mbo] 0: learning_rate=0.0275; num_leaves=48; feature_fraction=0.8; bagging_fraction=0.941; bagging_freq=10; min_data_in_leaf=160; lambda_l1=4.1; lambda_l2=1.54; min_gain_to_split=0.295; max_depth=2; min_sum_hessian_in_leaf=8.02; scale_pos_weight=38.9 : y = 0.926 : 108.5 secs : initdesign\n",
            "\n",
            "[mbo] 0: learning_rate=0.0177; num_leaves=90; feature_fraction=0.692; bagging_fraction=0.953; bagging_freq=10; min_data_in_leaf=177; lambda_l1=2.63; lambda_l2=5.07; min_gain_to_split=0.056; max_depth=10; min_sum_hessian_in_leaf=2.52; scale_pos_weight=146 : y = 0.926 : 280.1 secs : initdesign\n",
            "\n",
            "[mbo] 0: learning_rate=0.0237; num_leaves=67; feature_fraction=0.773; bagging_fraction=0.813; bagging_freq=3; min_data_in_leaf=75; lambda_l1=7.94; lambda_l2=1.01; min_gain_to_split=0.0914; max_depth=8; min_sum_hessian_in_leaf=9.41; scale_pos_weight=17.7 : y = 0.929 : 275.6 secs : initdesign\n",
            "\n",
            "[mbo] 0: learning_rate=0.0186; num_leaves=80; feature_fraction=0.749; bagging_fraction=0.898; bagging_freq=10; min_data_in_leaf=222; lambda_l1=7.41; lambda_l2=2.76; min_gain_to_split=0.288; max_depth=2; min_sum_hessian_in_leaf=2.38; scale_pos_weight=190 : y = 0.923 : 106.7 secs : initdesign\n",
            "\n",
            "[mbo] 0: learning_rate=0.0112; num_leaves=74; feature_fraction=0.778; bagging_fraction=0.73; bagging_freq=2; min_data_in_leaf=148; lambda_l1=6.48; lambda_l2=6.27; min_gain_to_split=0.277; max_depth=9; min_sum_hessian_in_leaf=4.25; scale_pos_weight=74.5 : y = 0.928 : 296.3 secs : initdesign\n",
            "\n",
            "[mbo] 0: learning_rate=0.0292; num_leaves=72; feature_fraction=0.608; bagging_fraction=0.841; bagging_freq=2; min_data_in_leaf=57; lambda_l1=1.02; lambda_l2=7.42; min_gain_to_split=0.168; max_depth=10; min_sum_hessian_in_leaf=6.18; scale_pos_weight=113 : y = 0.924 : 350.1 secs : initdesign\n",
            "\n",
            "[mbo] 0: learning_rate=0.0221; num_leaves=43; feature_fraction=0.896; bagging_fraction=0.788; bagging_freq=7; min_data_in_leaf=89; lambda_l1=3.22; lambda_l2=4.65; min_gain_to_split=0.287; max_depth=0; min_sum_hessian_in_leaf=2.19; scale_pos_weight=144 : y = 0.927 : 231.9 secs : initdesign\n",
            "\n",
            "[mbo] 0: learning_rate=0.0284; num_leaves=34; feature_fraction=0.815; bagging_fraction=0.98; bagging_freq=7; min_data_in_leaf=95; lambda_l1=3.04; lambda_l2=3.98; min_gain_to_split=0.0106; max_depth=-1; min_sum_hessian_in_leaf=8.29; scale_pos_weight=166 : y = 0.927 : 222.6 secs : initdesign\n",
            "\n",
            "[mbo] 0: learning_rate=0.023; num_leaves=55; feature_fraction=0.879; bagging_fraction=0.832; bagging_freq=9; min_data_in_leaf=241; lambda_l1=9.56; lambda_l2=3.74; min_gain_to_split=0.0624; max_depth=7; min_sum_hessian_in_leaf=0.014; scale_pos_weight=24.3 : y = 0.929 : 214.9 secs : initdesign\n",
            "\n",
            "[mbo] 0: learning_rate=0.0124; num_leaves=33; feature_fraction=0.628; bagging_fraction=0.907; bagging_freq=4; min_data_in_leaf=131; lambda_l1=1.19; lambda_l2=4.94; min_gain_to_split=0.149; max_depth=1; min_sum_hessian_in_leaf=7.25; scale_pos_weight=10 : y = 0.918 : 137.0 secs : initdesign\n",
            "\n",
            "[mbo] 0: learning_rate=0.0102; num_leaves=40; feature_fraction=0.842; bagging_fraction=0.904; bagging_freq=0; min_data_in_leaf=243; lambda_l1=0.572; lambda_l2=9.22; min_gain_to_split=0.14; max_depth=5; min_sum_hessian_in_leaf=6.49; scale_pos_weight=172 : y = 0.925 : 129.1 secs : initdesign\n",
            "\n",
            "[mbo] 0: learning_rate=0.0259; num_leaves=92; feature_fraction=0.874; bagging_fraction=0.725; bagging_freq=5; min_data_in_leaf=182; lambda_l1=1.31; lambda_l2=9.74; min_gain_to_split=0.114; max_depth=9; min_sum_hessian_in_leaf=9.94; scale_pos_weight=47.7 : y = 0.928 : 248.7 secs : initdesign\n",
            "\n",
            "[mbo] 0: learning_rate=0.0104; num_leaves=63; feature_fraction=0.833; bagging_fraction=0.913; bagging_freq=5; min_data_in_leaf=108; lambda_l1=8.54; lambda_l2=2.47; min_gain_to_split=0.0144; max_depth=5; min_sum_hessian_in_leaf=0.344; scale_pos_weight=174 : y = 0.925 : 186.6 secs : initdesign\n",
            "\n",
            "[mbo] 0: learning_rate=0.0289; num_leaves=34; feature_fraction=0.855; bagging_fraction=0.851; bagging_freq=1; min_data_in_leaf=100; lambda_l1=0.77; lambda_l2=9.48; min_gain_to_split=0.0349; max_depth=8; min_sum_hessian_in_leaf=0.45; scale_pos_weight=67.7 : y = 0.927 : 165.0 secs : initdesign\n",
            "\n",
            "[mbo] 0: learning_rate=0.013; num_leaves=51; feature_fraction=0.603; bagging_fraction=0.775; bagging_freq=6; min_data_in_leaf=59; lambda_l1=8.9; lambda_l2=8.35; min_gain_to_split=0.128; max_depth=6; min_sum_hessian_in_leaf=9.22; scale_pos_weight=196 : y = 0.924 : 229.6 secs : initdesign\n",
            "\n",
            "[mbo] 0: learning_rate=0.021; num_leaves=95; feature_fraction=0.67; bagging_fraction=0.746; bagging_freq=9; min_data_in_leaf=145; lambda_l1=7.19; lambda_l2=9.8; min_gain_to_split=0.188; max_depth=0; min_sum_hessian_in_leaf=1.53; scale_pos_weight=43.6 : y = 0.929 : 285.6 secs : initdesign\n",
            "\n",
            "[mbo] 0: learning_rate=0.0151; num_leaves=89; feature_fraction=0.744; bagging_fraction=0.759; bagging_freq=1; min_data_in_leaf=64; lambda_l1=0.133; lambda_l2=0.0886; min_gain_to_split=0.0757; max_depth=6; min_sum_hessian_in_leaf=9.06; scale_pos_weight=184 : y = 0.923 : 148.3 secs : initdesign\n",
            "\n",
            "[mbo] 0: learning_rate=0.0263; num_leaves=37; feature_fraction=0.727; bagging_fraction=0.743; bagging_freq=5; min_data_in_leaf=206; lambda_l1=9.25; lambda_l2=1.41; min_gain_to_split=0.153; max_depth=10; min_sum_hessian_in_leaf=1.69; scale_pos_weight=195 : y = 0.927 : 221.2 secs : initdesign\n",
            "\n",
            "[mbo] 0: learning_rate=0.0141; num_leaves=80; feature_fraction=0.865; bagging_fraction=0.95; bagging_freq=8; min_data_in_leaf=52; lambda_l1=1.51; lambda_l2=0.714; min_gain_to_split=0.234; max_depth=8; min_sum_hessian_in_leaf=3.71; scale_pos_weight=28.8 : y = 0.925 : 249.4 secs : initdesign\n",
            "\n",
            "[mbo] 0: learning_rate=0.0201; num_leaves=54; feature_fraction=0.827; bagging_fraction=0.799; bagging_freq=7; min_data_in_leaf=247; lambda_l1=0.332; lambda_l2=3.24; min_gain_to_split=0.0062; max_depth=-1; min_sum_hessian_in_leaf=0.899; scale_pos_weight=141 : y = 0.929 : 260.1 secs : initdesign\n",
            "\n",
            "[mbo] 0: learning_rate=0.0243; num_leaves=78; feature_fraction=0.686; bagging_fraction=0.997; bagging_freq=1; min_data_in_leaf=200; lambda_l1=6.86; lambda_l2=5.71; min_gain_to_split=0.254; max_depth=4; min_sum_hessian_in_leaf=9.71; scale_pos_weight=93.3 : y = 0.926 : 115.7 secs : initdesign\n",
            "\n",
            "Saved the current state after iteration 1 in the file bayesiana.RDATA.\n",
            "\n",
            "Sat Nov 01 06:01:15 PM 2025 AUC 0.929218788730169\n",
            "\n",
            "[mbo] 1: learning_rate=0.0142; num_leaves=82; feature_fraction=0.723; bagging_fraction=0.846; bagging_freq=1; min_data_in_leaf=194; lambda_l1=5.77; lambda_l2=4.52; min_gain_to_split=0.182; max_depth=-1; min_sum_hessian_in_leaf=2.58; scale_pos_weight=90.8 : y = 0.929 : 241.0 secs : infill_ei\n",
            "\n",
            "Sat Nov 01 06:06:24 PM 2025 AUC 0.930706741890557\n",
            "\n",
            "[mbo] 2: learning_rate=0.011; num_leaves=53; feature_fraction=0.6; bagging_fraction=0.79; bagging_freq=5; min_data_in_leaf=134; lambda_l1=6.53; lambda_l2=9.36; min_gain_to_split=0.0341; max_depth=0; min_sum_hessian_in_leaf=7.83; scale_pos_weight=13 : y = 0.931 : 308.0 secs : infill_ei\n",
            "\n",
            "Sat Nov 01 06:10:58 PM 2025 AUC 0.930508220351698\n",
            "\n",
            "[mbo] 3: learning_rate=0.0112; num_leaves=52; feature_fraction=0.692; bagging_fraction=0.791; bagging_freq=5; min_data_in_leaf=235; lambda_l1=6.68; lambda_l2=6.9; min_gain_to_split=0.034; max_depth=0; min_sum_hessian_in_leaf=9.08; scale_pos_weight=10.3 : y = 0.931 : 273.9 secs : infill_ei\n",
            "\n",
            "Saved the current state after iteration 4 in the file bayesiana.RDATA.\n",
            "\n",
            "Sat Nov 01 06:17:22 PM 2025 AUC 0.931178496157047\n",
            "\n",
            "[mbo] 4: learning_rate=0.0104; num_leaves=82; feature_fraction=0.609; bagging_fraction=0.935; bagging_freq=4; min_data_in_leaf=169; lambda_l1=3.18; lambda_l2=6.66; min_gain_to_split=0.0282; max_depth=0; min_sum_hessian_in_leaf=8.88; scale_pos_weight=14.7 : y = 0.931 : 383.1 secs : infill_ei\n",
            "\n",
            "Sat Nov 01 06:22:58 PM 2025 AUC 0.931307471650922\n",
            "\n",
            "[mbo] 5: learning_rate=0.01; num_leaves=70; feature_fraction=0.674; bagging_fraction=0.895; bagging_freq=4; min_data_in_leaf=165; lambda_l1=0.0313; lambda_l2=9.2; min_gain_to_split=0.0696; max_depth=0; min_sum_hessian_in_leaf=6.09; scale_pos_weight=10.5 : y = 0.931 : 334.3 secs : infill_ei\n",
            "\n",
            "Saved the current state after iteration 6 in the file bayesiana.RDATA.\n",
            "\n",
            "Sat Nov 01 06:27:49 PM 2025 AUC 0.930851945242628\n",
            "\n",
            "[mbo] 6: learning_rate=0.0113; num_leaves=53; feature_fraction=0.647; bagging_fraction=0.933; bagging_freq=7; min_data_in_leaf=168; lambda_l1=1.11; lambda_l2=7.66; min_gain_to_split=0.00608; max_depth=0; min_sum_hessian_in_leaf=6.55; scale_pos_weight=17.5 : y = 0.931 : 289.9 secs : infill_ei\n",
            "\n",
            "Sat Nov 01 06:33:28 PM 2025 AUC 0.931080862130016\n",
            "\n",
            "[mbo] 7: learning_rate=0.0121; num_leaves=91; feature_fraction=0.685; bagging_fraction=0.816; bagging_freq=4; min_data_in_leaf=189; lambda_l1=1.05; lambda_l2=9.97; min_gain_to_split=0.0121; max_depth=0; min_sum_hessian_in_leaf=9.83; scale_pos_weight=11.7 : y = 0.931 : 338.0 secs : infill_ei\n",
            "\n",
            "Saved the current state after iteration 8 in the file bayesiana.RDATA.\n",
            "\n",
            "Sat Nov 01 06:37:56 PM 2025 AUC 0.93002112799946\n",
            "\n",
            "[mbo] 8: learning_rate=0.016; num_leaves=78; feature_fraction=0.626; bagging_fraction=0.898; bagging_freq=1; min_data_in_leaf=217; lambda_l1=1.48; lambda_l2=8.72; min_gain_to_split=0.107; max_depth=0; min_sum_hessian_in_leaf=7.11; scale_pos_weight=10.6 : y = 0.93 : 266.9 secs : infill_ei\n",
            "\n",
            "Sat Nov 01 06:43:26 PM 2025 AUC 0.930563741314859\n",
            "\n",
            "[mbo] 9: learning_rate=0.01; num_leaves=80; feature_fraction=0.749; bagging_fraction=0.884; bagging_freq=6; min_data_in_leaf=115; lambda_l1=1.69; lambda_l2=7.53; min_gain_to_split=0.105; max_depth=0; min_sum_hessian_in_leaf=8.99; scale_pos_weight=11.5 : y = 0.931 : 329.0 secs : infill_ei\n",
            "\n",
            "Sat Nov 01 06:49:03 PM 2025 AUC 0.929428465641917\n",
            "\n",
            "[mbo] 10: learning_rate=0.01; num_leaves=83; feature_fraction=0.688; bagging_fraction=0.85; bagging_freq=4; min_data_in_leaf=179; lambda_l1=2.4; lambda_l2=5.77; min_gain_to_split=0.00432; max_depth=0; min_sum_hessian_in_leaf=5.9; scale_pos_weight=56.2 : y = 0.929 : 335.9 secs : infill_ei\n",
            "\n",
            "Saved the current state after iteration 11 in the file bayesiana.RDATA.\n",
            "\n",
            "Sat Nov 01 06:54:52 PM 2025 AUC 0.931168988291751\n",
            "\n",
            "[mbo] 11: learning_rate=0.0123; num_leaves=87; feature_fraction=0.662; bagging_fraction=0.921; bagging_freq=4; min_data_in_leaf=124; lambda_l1=7.91; lambda_l2=9.93; min_gain_to_split=0.029; max_depth=0; min_sum_hessian_in_leaf=9.34; scale_pos_weight=10.3 : y = 0.931 : 348.3 secs : infill_ei\n",
            "\n",
            "Sat Nov 01 07:01:04 PM 2025 AUC 0.930475051440163\n",
            "\n",
            "[mbo] 12: learning_rate=0.0114; num_leaves=57; feature_fraction=0.661; bagging_fraction=0.996; bagging_freq=2; min_data_in_leaf=93; lambda_l1=3.58; lambda_l2=2.13; min_gain_to_split=0.0124; max_depth=0; min_sum_hessian_in_leaf=9.95; scale_pos_weight=10.1 : y = 0.93 : 371.3 secs : infill_ei\n",
            "\n",
            "Saved the current state after iteration 13 in the file bayesiana.RDATA.\n",
            "\n",
            "Sat Nov 01 07:05:41 PM 2025 AUC 0.92937692836908\n",
            "\n",
            "[mbo] 13: learning_rate=0.0147; num_leaves=62; feature_fraction=0.849; bagging_fraction=0.859; bagging_freq=9; min_data_in_leaf=68; lambda_l1=2.67; lambda_l2=6.85; min_gain_to_split=0.296; max_depth=-1; min_sum_hessian_in_leaf=0.432; scale_pos_weight=43.5 : y = 0.929 : 276.2 secs : infill_ei\n",
            "\n",
            "Sat Nov 01 07:11:06 PM 2025 AUC 0.93102261042827\n",
            "\n",
            "[mbo] 14: learning_rate=0.0102; num_leaves=51; feature_fraction=0.645; bagging_fraction=0.892; bagging_freq=3; min_data_in_leaf=129; lambda_l1=8.65; lambda_l2=9.88; min_gain_to_split=0.0177; max_depth=0; min_sum_hessian_in_leaf=0.922; scale_pos_weight=10.2 : y = 0.931 : 323.3 secs : infill_ei\n",
            "\n",
            "Saved the current state after iteration 15 in the file bayesiana.RDATA.\n",
            "\n",
            "Sat Nov 01 07:16:26 PM 2025 AUC 0.931142931816778\n",
            "\n",
            "[mbo] 15: learning_rate=0.0108; num_leaves=48; feature_fraction=0.784; bagging_fraction=0.936; bagging_freq=3; min_data_in_leaf=144; lambda_l1=5.03; lambda_l2=9.59; min_gain_to_split=0.0369; max_depth=0; min_sum_hessian_in_leaf=9.85; scale_pos_weight=16.6 : y = 0.931 : 318.8 secs : infill_ei\n",
            "\n",
            "Sat Nov 01 07:21:47 PM 2025 AUC 0.93035790242013\n",
            "\n",
            "[mbo] 16: learning_rate=0.0103; num_leaves=62; feature_fraction=0.856; bagging_fraction=0.883; bagging_freq=4; min_data_in_leaf=189; lambda_l1=6.1; lambda_l2=9.83; min_gain_to_split=0.02; max_depth=0; min_sum_hessian_in_leaf=2.74; scale_pos_weight=10.1 : y = 0.93 : 320.6 secs : infill_ei\n",
            "\n",
            "Saved the current state after iteration 17 in the file bayesiana.RDATA.\n",
            "\n",
            "Sat Nov 01 07:27:48 PM 2025 AUC 0.930549635061026\n",
            "\n",
            "[mbo] 17: learning_rate=0.01; num_leaves=86; feature_fraction=0.65; bagging_fraction=0.952; bagging_freq=4; min_data_in_leaf=93; lambda_l1=8.33; lambda_l2=9.96; min_gain_to_split=0.287; max_depth=0; min_sum_hessian_in_leaf=3.4; scale_pos_weight=10.9 : y = 0.931 : 359.8 secs : infill_ei\n",
            "\n",
            "Sat Nov 01 07:32:38 PM 2025 AUC 0.930594413612125\n",
            "\n",
            "[mbo] 18: learning_rate=0.0107; num_leaves=42; feature_fraction=0.673; bagging_fraction=0.964; bagging_freq=4; min_data_in_leaf=164; lambda_l1=0.336; lambda_l2=8.6; min_gain_to_split=0.256; max_depth=0; min_sum_hessian_in_leaf=9.74; scale_pos_weight=11.1 : y = 0.931 : 288.2 secs : infill_ei\n",
            "\n",
            "Saved the current state after iteration 19 in the file bayesiana.RDATA.\n",
            "\n",
            "Sat Nov 01 07:37:35 PM 2025 AUC 0.928086507325151\n",
            "\n",
            "[mbo] 19: learning_rate=0.0106; num_leaves=96; feature_fraction=0.895; bagging_fraction=0.702; bagging_freq=7; min_data_in_leaf=64; lambda_l1=7.63; lambda_l2=9.21; min_gain_to_split=0.172; max_depth=-1; min_sum_hessian_in_leaf=3.56; scale_pos_weight=133 : y = 0.928 : 295.1 secs : infill_ei\n",
            "\n",
            "Sat Nov 01 07:43:27 PM 2025 AUC 0.929786464339172\n",
            "\n",
            "[mbo] 20: learning_rate=0.01; num_leaves=49; feature_fraction=0.677; bagging_fraction=0.944; bagging_freq=2; min_data_in_leaf=122; lambda_l1=3.92; lambda_l2=8.35; min_gain_to_split=0.125; max_depth=0; min_sum_hessian_in_leaf=1.4; scale_pos_weight=23.1 : y = 0.93 : 349.4 secs : infill_ei\n",
            "\n",
            "Saved the current state after iteration 21 in the file bayesiana.RDATA.\n",
            "\n",
            "Sat Nov 01 07:48:33 PM 2025 AUC 0.931415117569891\n",
            "\n",
            "[mbo] 21: learning_rate=0.0101; num_leaves=51; feature_fraction=0.706; bagging_fraction=0.978; bagging_freq=5; min_data_in_leaf=157; lambda_l1=9.63; lambda_l2=7.88; min_gain_to_split=0.0756; max_depth=0; min_sum_hessian_in_leaf=6.19; scale_pos_weight=10.4 : y = 0.931 : 304.3 secs : infill_ei\n",
            "\n",
            "Sat Nov 01 07:53:29 PM 2025 AUC 0.92838594310963\n",
            "\n",
            "[mbo] 22: learning_rate=0.0205; num_leaves=73; feature_fraction=0.61; bagging_fraction=0.729; bagging_freq=7; min_data_in_leaf=192; lambda_l1=0.548; lambda_l2=8.75; min_gain_to_split=0.171; max_depth=-1; min_sum_hessian_in_leaf=6.32; scale_pos_weight=19.5 : y = 0.928 : 294.8 secs : infill_ei\n",
            "\n",
            "Saved the current state after iteration 23 in the file bayesiana.RDATA.\n",
            "\n",
            "Sat Nov 01 07:58:52 PM 2025 AUC 0.931239005644905\n",
            "\n",
            "[mbo] 23: learning_rate=0.0105; num_leaves=56; feature_fraction=0.67; bagging_fraction=0.958; bagging_freq=4; min_data_in_leaf=165; lambda_l1=8.86; lambda_l2=8.59; min_gain_to_split=0.165; max_depth=-1; min_sum_hessian_in_leaf=9.2; scale_pos_weight=11.9 : y = 0.931 : 321.2 secs : infill_ei\n",
            "\n",
            "Sat Nov 01 08:03:52 PM 2025 AUC 0.929304811187644\n",
            "\n",
            "[mbo] 24: learning_rate=0.0101; num_leaves=55; feature_fraction=0.66; bagging_fraction=0.902; bagging_freq=5; min_data_in_leaf=181; lambda_l1=9.18; lambda_l2=9.86; min_gain_to_split=0.0208; max_depth=0; min_sum_hessian_in_leaf=5.25; scale_pos_weight=10.4 : y = 0.929 : 299.2 secs : infill_ei\n",
            "\n",
            "Saved the current state after iteration 25 in the file bayesiana.RDATA.\n",
            "\n",
            "Sat Nov 01 08:08:56 PM 2025 AUC 0.930334169670157\n",
            "\n",
            "[mbo] 25: learning_rate=0.0162; num_leaves=52; feature_fraction=0.801; bagging_fraction=0.983; bagging_freq=4; min_data_in_leaf=76; lambda_l1=8.42; lambda_l2=5.2; min_gain_to_split=0.0186; max_depth=0; min_sum_hessian_in_leaf=1.45; scale_pos_weight=10.7 : y = 0.93 : 301.8 secs : infill_ei\n",
            "\n",
            "Sat Nov 01 08:14:06 PM 2025 AUC 0.931175333154728\n",
            "\n",
            "[mbo] 26: learning_rate=0.0149; num_leaves=50; feature_fraction=0.692; bagging_fraction=0.986; bagging_freq=4; min_data_in_leaf=130; lambda_l1=6.8; lambda_l2=7.53; min_gain_to_split=0.0757; max_depth=0; min_sum_hessian_in_leaf=8.63; scale_pos_weight=10.2 : y = 0.931 : 309.3 secs : infill_ei\n",
            "\n",
            "Saved the current state after iteration 27 in the file bayesiana.RDATA.\n",
            "\n",
            "Sat Nov 01 08:19:38 PM 2025 AUC 0.929565843672373\n",
            "\n",
            "[mbo] 27: learning_rate=0.0102; num_leaves=63; feature_fraction=0.707; bagging_fraction=0.999; bagging_freq=5; min_data_in_leaf=128; lambda_l1=6.44; lambda_l2=9.15; min_gain_to_split=0.29; max_depth=0; min_sum_hessian_in_leaf=6.8; scale_pos_weight=15 : y = 0.93 : 329.6 secs : infill_ei\n",
            "\n",
            "Sat Nov 01 08:24:48 PM 2025 AUC 0.930908851328205\n",
            "\n",
            "[mbo] 28: learning_rate=0.0104; num_leaves=57; feature_fraction=0.802; bagging_fraction=0.824; bagging_freq=3; min_data_in_leaf=151; lambda_l1=8.24; lambda_l2=6.64; min_gain_to_split=0.0385; max_depth=0; min_sum_hessian_in_leaf=9.41; scale_pos_weight=12.5 : y = 0.931 : 309.2 secs : infill_ei\n",
            "\n",
            "Saved the current state after iteration 29 in the file bayesiana.RDATA.\n",
            "\n",
            "Sat Nov 01 08:29:55 PM 2025 AUC 0.930743570336875\n",
            "\n",
            "[mbo] 29: learning_rate=0.0117; num_leaves=54; feature_fraction=0.799; bagging_fraction=0.948; bagging_freq=5; min_data_in_leaf=164; lambda_l1=8.63; lambda_l2=8.37; min_gain_to_split=0.238; max_depth=-1; min_sum_hessian_in_leaf=8.75; scale_pos_weight=18.6 : y = 0.931 : 304.7 secs : infill_ei\n",
            "\n",
            "Sat Nov 01 08:35:43 PM 2025 AUC 0.930541738418456\n",
            "\n",
            "[mbo] 30: learning_rate=0.0115; num_leaves=70; feature_fraction=0.737; bagging_fraction=0.998; bagging_freq=4; min_data_in_leaf=130; lambda_l1=5.87; lambda_l2=3.48; min_gain_to_split=0.171; max_depth=-1; min_sum_hessian_in_leaf=9.23; scale_pos_weight=10.1 : y = 0.931 : 346.6 secs : infill_ei\n",
            "\n",
            "Saved the current state after iteration 31 in the file bayesiana.RDATA.\n",
            "\n",
            "Sat Nov 01 08:41:51 PM 2025 AUC 0.931268195875437\n",
            "\n",
            "[mbo] 31: learning_rate=0.0105; num_leaves=90; feature_fraction=0.678; bagging_fraction=0.937; bagging_freq=5; min_data_in_leaf=162; lambda_l1=0.398; lambda_l2=6.36; min_gain_to_split=0.0444; max_depth=0; min_sum_hessian_in_leaf=9.12; scale_pos_weight=10 : y = 0.931 : 366.1 secs : infill_ei\n",
            "\n",
            "Sat Nov 01 08:46:33 PM 2025 AUC 0.930626423512178\n",
            "\n",
            "[mbo] 32: learning_rate=0.0102; num_leaves=47; feature_fraction=0.868; bagging_fraction=0.936; bagging_freq=7; min_data_in_leaf=136; lambda_l1=9.95; lambda_l2=7.01; min_gain_to_split=0.0651; max_depth=0; min_sum_hessian_in_leaf=0.371; scale_pos_weight=10.1 : y = 0.931 : 280.9 secs : infill_ei\n",
            "\n",
            "Saved the current state after iteration 33 in the file bayesiana.RDATA.\n",
            "\n",
            "Sat Nov 01 08:50:51 PM 2025 AUC 0.929858497672711\n",
            "\n",
            "[mbo] 33: learning_rate=0.0162; num_leaves=38; feature_fraction=0.709; bagging_fraction=0.916; bagging_freq=5; min_data_in_leaf=92; lambda_l1=8.58; lambda_l2=7.39; min_gain_to_split=0.156; max_depth=-1; min_sum_hessian_in_leaf=4.87; scale_pos_weight=21.4 : y = 0.93 : 256.3 secs : infill_ei\n",
            "\n",
            "Sat Nov 01 08:55:46 PM 2025 AUC 0.930141329669988\n",
            "\n",
            "[mbo] 34: learning_rate=0.0108; num_leaves=47; feature_fraction=0.648; bagging_fraction=0.973; bagging_freq=6; min_data_in_leaf=205; lambda_l1=2.69; lambda_l2=9.91; min_gain_to_split=0.198; max_depth=-1; min_sum_hessian_in_leaf=7.13; scale_pos_weight=27.5 : y = 0.93 : 293.2 secs : infill_ei\n",
            "\n",
            "Sat Nov 01 09:00:39 PM 2025 AUC 0.930885153377289\n",
            "\n",
            "[mbo] 35: learning_rate=0.0101; num_leaves=55; feature_fraction=0.761; bagging_fraction=0.95; bagging_freq=8; min_data_in_leaf=166; lambda_l1=7.28; lambda_l2=4.01; min_gain_to_split=0.114; max_depth=-1; min_sum_hessian_in_leaf=1.08; scale_pos_weight=18.8 : y = 0.931 : 291.5 secs : infill_ei\n",
            "\n",
            "Saved the final state in the file bayesiana.RDATA\n",
            "\n",
            "Saved the final state in the file bayesiana.RDATA\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tb_bayesiana <- as.data.table(bayesiana_salida$opt.path)\n",
        "colnames( tb_bayesiana)"
      ],
      "metadata": {
        "id": "ssk5nnMk6INK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "070d6b0b-67be-468f-fb5b-dd9ec1e0eadd"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>\n",
              ".list-inline {list-style: none; margin:0; padding: 0}\n",
              ".list-inline>li {display: inline-block}\n",
              ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
              "</style>\n",
              "<ol class=list-inline><li>'learning_rate'</li><li>'num_leaves'</li><li>'feature_fraction'</li><li>'bagging_fraction'</li><li>'bagging_freq'</li><li>'min_data_in_leaf'</li><li>'lambda_l1'</li><li>'lambda_l2'</li><li>'min_gain_to_split'</li><li>'max_depth'</li><li>'min_sum_hessian_in_leaf'</li><li>'scale_pos_weight'</li><li>'y'</li><li>'dob'</li><li>'eol'</li><li>'error.message'</li><li>'exec.time'</li><li>'ei'</li><li>'error.model'</li><li>'train.time'</li><li>'prop.type'</li><li>'propose.time'</li><li>'se'</li><li>'mean'</li></ol>\n"
            ],
            "text/markdown": "1. 'learning_rate'\n2. 'num_leaves'\n3. 'feature_fraction'\n4. 'bagging_fraction'\n5. 'bagging_freq'\n6. 'min_data_in_leaf'\n7. 'lambda_l1'\n8. 'lambda_l2'\n9. 'min_gain_to_split'\n10. 'max_depth'\n11. 'min_sum_hessian_in_leaf'\n12. 'scale_pos_weight'\n13. 'y'\n14. 'dob'\n15. 'eol'\n16. 'error.message'\n17. 'exec.time'\n18. 'ei'\n19. 'error.model'\n20. 'train.time'\n21. 'prop.type'\n22. 'propose.time'\n23. 'se'\n24. 'mean'\n\n\n",
            "text/latex": "\\begin{enumerate*}\n\\item 'learning\\_rate'\n\\item 'num\\_leaves'\n\\item 'feature\\_fraction'\n\\item 'bagging\\_fraction'\n\\item 'bagging\\_freq'\n\\item 'min\\_data\\_in\\_leaf'\n\\item 'lambda\\_l1'\n\\item 'lambda\\_l2'\n\\item 'min\\_gain\\_to\\_split'\n\\item 'max\\_depth'\n\\item 'min\\_sum\\_hessian\\_in\\_leaf'\n\\item 'scale\\_pos\\_weight'\n\\item 'y'\n\\item 'dob'\n\\item 'eol'\n\\item 'error.message'\n\\item 'exec.time'\n\\item 'ei'\n\\item 'error.model'\n\\item 'train.time'\n\\item 'prop.type'\n\\item 'propose.time'\n\\item 'se'\n\\item 'mean'\n\\end{enumerate*}\n",
            "text/plain": [
              " [1] \"learning_rate\"           \"num_leaves\"             \n",
              " [3] \"feature_fraction\"        \"bagging_fraction\"       \n",
              " [5] \"bagging_freq\"            \"min_data_in_leaf\"       \n",
              " [7] \"lambda_l1\"               \"lambda_l2\"              \n",
              " [9] \"min_gain_to_split\"       \"max_depth\"              \n",
              "[11] \"min_sum_hessian_in_leaf\" \"scale_pos_weight\"       \n",
              "[13] \"y\"                       \"dob\"                    \n",
              "[15] \"eol\"                     \"error.message\"          \n",
              "[17] \"exec.time\"               \"ei\"                     \n",
              "[19] \"error.model\"             \"train.time\"             \n",
              "[21] \"prop.type\"               \"propose.time\"           \n",
              "[23] \"se\"                      \"mean\"                   "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# almaceno los resultados de la Bayesian Optimization\n",
        "# y capturo los mejores hiperparametros encontrados\n",
        "\n",
        "tb_bayesiana <- as.data.table(bayesiana_salida$opt.path)\n",
        "\n",
        "tb_bayesiana[, iter := .I]\n",
        "\n",
        "# ordeno en forma descendente por AUC = y\n",
        "setorder(tb_bayesiana, -y)\n",
        "\n",
        "# grabo para eventualmente poder utilizarlos en OTRA corrida\n",
        "fwrite( tb_bayesiana,\n",
        "  file= \"BO_log.txt\",\n",
        "  sep= \"\\t\"\n",
        ")\n",
        "\n",
        "# los mejores hiperparámetros son los que quedaron en el registro 1 de la tabla\n",
        "PARAM$out$lgbm$mejores_hiperparametros <- tb_bayesiana[\n",
        "  1, # el primero es el de mejor AUC\n",
        "  setdiff(colnames(tb_bayesiana),\n",
        "    c(\"y\",\"dob\",\"eol\",\"error.message\",\"exec.time\",\"ei\",\"error.model\",\n",
        "      \"train.time\",\"prop.type\",\"propose.time\",\"se\",\"mean\",\"iter\")),\n",
        "  with= FALSE\n",
        "]\n",
        "\n",
        "\n",
        "PARAM$out$lgbm$y <- tb_bayesiana[1, y]\n"
      ],
      "metadata": {
        "id": "u4zq-vknhjGc"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "write_yaml( PARAM, file=\"PARAM.yml\")"
      ],
      "metadata": {
        "id": "E8v2eA427N8e"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(PARAM$out$lgbm$mejores_hiperparametros)\n",
        "print(PARAM$out$lgbm$y)"
      ],
      "metadata": {
        "id": "iBTWexVU7PGC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce290247-63f3-48c8-f9c5-31fd8fcba8f4"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   learning_rate num_leaves feature_fraction bagging_fraction bagging_freq\n",
            "           <num>      <int>            <num>            <num>        <int>\n",
            "1:    0.01005514         51        0.7063844        0.9783509            5\n",
            "   min_data_in_leaf lambda_l1 lambda_l2 min_gain_to_split max_depth\n",
            "              <int>     <num>     <num>             <num>     <int>\n",
            "1:              157  9.632635  7.882127        0.07555869         0\n",
            "   min_sum_hessian_in_leaf scale_pos_weight\n",
            "                     <num>            <num>\n",
            "1:                6.185269         10.39048\n",
            "[1] 0.9314151\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Primer Corrida \"Rapida y Liviana\"\n",
        "10 Iteraciones Bayesianas\n",
        "\n",
        "- Experimento: 5940\n",
        "- Semilla: 100007\n",
        "\n",
        "\n",
        "PARAM$hypeparametertuning$hs <- makeParamSet\n",
        "(\n",
        "  makeNumericParam(\"learning_rate\", lower = 0.05, upper = 0.2),\n",
        "\n",
        "  makeIntegerParam(\"num_leaves\", lower = 16L, upper = 128L),\n",
        "\n",
        "  makeIntegerParam(\"max_depth\", lower = 3L, upper = 8L),\n",
        "\n",
        "  makeNumericParam(\"feature_fraction\", lower = 0.6, upper = 1.0),\n",
        "\n",
        "  makeIntegerParam(\"min_data_in_leaf\", lower = 50L, upper = 1000L)\n",
        ")\n",
        "\n",
        "- Rango chico, para entrenar rápido.\n",
        "- Ideal para datasets medianos (hasta 100k filas).\n",
        "- max_depth limita el tamaño de los árboles para evitar overfitting.\n",
        "- Con 10 iteraciones ya te da una buena base.\n",
        "\n",
        "**Resultados:**\n",
        "\n",
        "- learning_rate: 0.03173437\n",
        "- num_leaves: 178\n",
        "- feature_fraction: 0.5423336\n",
        "- bagging_fraction: 0.7422481\n",
        "- min_data_in_leaf: 548\n",
        "- Kaggle: Muy malo. Subajusto y da 5 o 6 en el public Leaderboard"
      ],
      "metadata": {
        "id": "443b6fALvf-8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Segunda Corrida\n",
        "\n",
        "PARAM$hiperparametertuninghs <- makeParamSet(\n",
        "\n",
        "  makeNumericParam(\"learning_rate\", lower = 0.01, upper = 0.05),\n",
        "  makeIntegerParam(\"num_leaves\", lower = 8L, upper = 64L),\n",
        "  makeNumericParam(\"feature_fraction\", lower = 0.5, upper = 0.9),\n",
        "  makeNumericParam(\"bagging_fraction\", lower = 0.7, upper = 1.0),\n",
        "  makeIntegerParam(\"min_data_in_leaf\", lower = 20L, upper = 200L)\n",
        ")\n",
        "\n",
        "Resultados\n",
        "\n",
        "    learning_rate: 0.01718392\n",
        "    num_leaves: 35\n",
        "    feature_fraction: 0.5969174\n",
        "    bagging_fraction: 0.9411694\n",
        "    min_data_in_leaf: 196\n",
        "    max_depth: -1\n",
        "    num_interactions: 1000\n",
        "\n",
        "    Kaggle:\n",
        "    - 100007: 55.267\n",
        "    - 454547: 53.147\n",
        "    - 737371: 50.527\n",
        "    - 999971: 54.367\n",
        "    - 999983: 52.247\n",
        "    - Promedio en Public Leaderboard por Semilla: 53.111\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RFj4yZ7JNgX0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tercer Corrida\n",
        "\n",
        "\n",
        "PARAM$hypeparametertuning$hs <- makeParamSet(\n",
        "  makeNumericParam(\"learning_rate\", lower = 0.01, upper = 0.05),\n",
        "  makeIntegerParam(\"num_leaves\", lower = 16L, upper = 128L),\n",
        "  makeNumericParam(\"feature_fraction\", lower = 0.5, upper = 0.9),\n",
        "  makeNumericParam(\"bagging_fraction\", lower = 0.7, upper = 1.0),\n",
        "  makeIntegerParam(\"min_data_in_leaf\", lower = 20L, upper = 200L),\n",
        "  makeNumericParam(\"lambda_l1\", lower = 0, upper = 10),\n",
        "  makeNumericParam(\"lambda_l2\", lower = 0, upper = 10),\n",
        "  makeNumericParam(\"min_gain_to_split\", lower = 0, upper = 0.5),\n",
        "  makeIntegerParam(\"max_depth\", lower = -1L, upper = 10L)\n",
        ")\n",
        "\n",
        "Resultados\n",
        "\n",
        "\n",
        "    learning_rate: 0.01390576\n",
        "    num_leaves: 47\n",
        "    feature_fraction: 0.5029553\n",
        "    bagging_fraction: 0.8387449\n",
        "    min_data_in_leaf: 158\n",
        "    lambda_l1: 1.389795\n",
        "    lambda_l2: 2.100884\n",
        "    max_depth: 7 (uso -1)\n",
        "    min_gain_to_split: 0.4506766\n",
        "    num_interactions: 1000\n",
        "\n",
        "    Probe con max_depth positivo y no me da buenos resultados (uso -1)\n",
        "\n",
        "    Kaggle:\n",
        "    - 100007: 54.827\n",
        "    - 454547: 55.207\n",
        "    - 737371: 55.227\n",
        "    - 999971: 54.157\n",
        "    - 999983: 55.677\n",
        "    - Promedio en Public Leaderboard por Semilla: 55.019\n"
      ],
      "metadata": {
        "id": "PxAC3OXHblM1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cuarta Corrida\n",
        "\n",
        "\n",
        "PARAM$hypeparametertuning$hs <- makeParamSet(\n",
        "  makeNumericParam(\"learning_rate\", lower = 0.01, upper = 0.03),\n",
        "  makeIntegerParam(\"num_leaves\", lower = 32L, upper = 96L),\n",
        "  makeNumericParam(\"feature_fraction\", lower = 0.6, upper = 0.9),\n",
        "  makeNumericParam(\"bagging_fraction\", lower = 0.7, upper = 1.0),\n",
        "  makeIntegerParam(\"bagging_freq\", lower = 0L, upper = 10L),\n",
        "  makeIntegerParam(\"min_data_in_leaf\", lower = 50L, upper = 250L),\n",
        "  makeNumericParam(\"lambda_l1\", lower = 0, upper = 10),\n",
        "  makeNumericParam(\"lambda_l2\", lower = 0, upper = 10),\n",
        "  makeNumericParam(\"min_gain_to_split\", lower = 0, upper = 0.3),\n",
        "  makeIntegerParam(\"max_depth\", lower = -1L, upper = 10L),\n",
        "  makeNumericParam(\"min_sum_hessian_in_leaf\", lower = 0.0001, upper = 10),\n",
        "  makeNumericParam(\"scale_pos_weight\", lower = 10, upper = 200)\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "Resultados\n",
        "\n",
        "    learning_rate:    0.01005514\n",
        "    num_leaves:       51\n",
        "    feature_fraction: 0.7063844\n",
        "    bagging_fraction: 0.9783509\n",
        "    bagging_freq:     5\n",
        "    min_data_in_leaf: 157\n",
        "    lambda_l1:        9.632635\n",
        "    lambda_l2:        7.882127\n",
        "    max_depth:        0\n",
        "    min_gain_to_split: 0.07555869\n",
        "    min_sum_hessian_in_leaf: 6.185269\n",
        "    scale_pos_weight: 10.39048\n",
        "    num_interactions: 1000\n",
        "\n",
        "\n",
        "Kaggle:\n",
        "\n",
        "    - 100007:  \n",
        "    - 454547:\n",
        "    - 737371:\n",
        "    - 999971:\n",
        "    - 999983:\n",
        "    - Promedio en Public Leaderboard por Semilla:\n"
      ],
      "metadata": {
        "id": "li3S9agrQFR_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "LzW7b3eVbfaG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3  Produccion"
      ],
      "metadata": {
        "id": "TKsVZmAnhwX-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Final Training\n",
        "Construyo el modelo final, que es uno solo, no hace ningun tipo de particion < training, validation, testing>]"
      ],
      "metadata": {
        "id": "RQ_C33Tr5B_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "setwd(\"/content/buckets/b1/exp\")\n",
        "experimento <- paste0(\"exp\", PARAM$experimento)\n",
        "dir.create(experimento, showWarnings= FALSE)\n",
        "setwd( paste0(\"/content/buckets/b1/exp/\", experimento ))"
      ],
      "metadata": {
        "id": "eDqfyA14hzwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Final Training Dataset\n",
        "\n",
        "Aqui esta la gran decision de en qué meses hago el Final Training\n",
        "<br> debo utilizar los mejores hiperparámetros que encontré en la optimización bayesiana"
      ],
      "metadata": {
        "id": "8qFmFivf5Iet"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# clase01\n",
        "dataset[, clase01 := ifelse(clase_ternaria %in% c(\"BAJA+1\", \"BAJA+2\"), 1L, 0L)]"
      ],
      "metadata": {
        "id": "lg5WVZncvc7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train <- dataset[foto_mes %in% c(202107)]"
      ],
      "metadata": {
        "id": "yc9QzXREv0xf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dejo los datos en el formato que necesita LightGBM\n",
        "\n",
        "dtrain <- lgb.Dataset(\n",
        "  data= data.matrix(dataset_train[, campos_buenos, with= FALSE]),\n",
        "  label= dataset_train[, clase01]\n",
        ")"
      ],
      "metadata": {
        "id": "thjdqEBLuvNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Final Training Hyperparameters"
      ],
      "metadata": {
        "id": "VNUa-WSz5Oqu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_final <- modifyList(PARAM$lgbm$param_fijos,\n",
        "  PARAM$out$lgbm$mejores_hiperparametros)\n",
        "\n",
        "param_final"
      ],
      "metadata": {
        "id": "FgCcvBfEwImu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training\n",
        "Genero el modelo final, siempre sobre TODOS los datos de  final_train, sin hacer ningun tipo de undersampling de la clase mayoritaria"
      ],
      "metadata": {
        "id": "TZIYn4l95TBH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# este punto es muy SUTIL  y será revisado en la Clase 05\n",
        "\n",
        "param_normalizado <- copy(param_final)\n",
        "param_normalizado$min_data_in_leaf <-  round(param_final$min_data_in_leaf / PARAM$trainingstrategy$undersampling)"
      ],
      "metadata": {
        "id": "vPLsd4mMRe4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  # entreno LightGBM\n",
        "\n",
        "  modelo_final <- lgb.train(\n",
        "    data= dtrain,\n",
        "    param= param_normalizado\n",
        "  )"
      ],
      "metadata": {
        "id": "WRI_-taRwOXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ahora imprimo la importancia de variables\n",
        "\n",
        "tb_importancia <- as.data.table(lgb.importance(modelo_final))\n",
        "archivo_importancia <- \"impo.txt\"\n",
        "\n",
        "fwrite(tb_importancia,\n",
        "  file= archivo_importancia,\n",
        "  sep= \"\\t\"\n",
        ")"
      ],
      "metadata": {
        "id": "_bkhnCvj0g3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# grabo a disco el modelo en un formato para seres humanos ... ponele ...\n",
        "\n",
        "lgb.save(modelo_final, \"modelo.txt\" )"
      ],
      "metadata": {
        "id": "lZ3sLmbh0kFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scoring"
      ],
      "metadata": {
        "id": "VEtp2--t5Ymg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aplico el modelo final a los datos del futuro"
      ],
      "metadata": {
        "id": "hI5008Mj5ZdI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# aplico el modelo a los datos sin clase\n",
        "dfuture <- dataset[foto_mes == 202109]\n",
        "\n",
        "# aplico el modelo a los datos nuevos\n",
        "prediccion <- predict(\n",
        "  modelo_final,\n",
        "  data.matrix(dfuture[, campos_buenos, with= FALSE])\n",
        ")"
      ],
      "metadata": {
        "id": "PimBY3N_0ryP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tabla Prediccion"
      ],
      "metadata": {
        "id": "D26rNRh55gpw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tabla de prediccion\n",
        "\n",
        "tb_prediccion <- dfuture[, list(numero_de_cliente)]\n",
        "tb_prediccion[, prob := prediccion ]\n",
        "\n",
        "# grabo las probabilidad del modelo\n",
        "fwrite(tb_prediccion,\n",
        "  file= \"prediccion.txt\",\n",
        "  sep= \"\\t\"\n",
        ")"
      ],
      "metadata": {
        "id": "RJwg7LHd11yu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kaggle Competition Submit"
      ],
      "metadata": {
        "id": "jOt4eG_55ltv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# genero archivos con los  \"envios\" mejores\n",
        "# suba TODOS los archivos a Kaggle\n",
        "\n",
        "# ordeno por probabilidad descendente\n",
        "setorder(tb_prediccion, -prob)\n",
        "\n",
        "dir.create(\"kaggle\")\n",
        "\n",
        "for (envios in PARAM$kaggle$cortes) {\n",
        "\n",
        "  tb_prediccion[, Predicted := 0L] # seteo inicial a 0\n",
        "  tb_prediccion[1:envios, Predicted := 1L] # marclo los primeros\n",
        "\n",
        "  archivo_kaggle <- paste0(\"./kaggle/KA\", PARAM$experimento, \"_\", envios, \".csv\")\n",
        "\n",
        "  # grabo el archivo\n",
        "  fwrite(tb_prediccion[, list(numero_de_cliente, Predicted)],\n",
        "    file= archivo_kaggle,\n",
        "    sep= \",\"\n",
        "  )\n",
        "\n",
        "  # subida a Kaggle, armo la linea de comando\n",
        "  comando <- \"kaggle competitions submit\"\n",
        "  competencia <- paste(\"-c\", PARAM$kaggle$competencia)\n",
        "  arch <- paste( \"-f\", archivo_kaggle)\n",
        "\n",
        "  mensaje <- paste0(\"-m 'envios=\", envios,\n",
        "  \"  semilla=\", PARAM$semilla_primigenia,\n",
        "    \"'\" )\n",
        "\n",
        "  linea <- paste( comando, competencia, arch, mensaje)\n",
        "\n",
        "  salida <- system(linea, intern=TRUE) # el submit a Kaggle\n",
        "  cat(salida, \"\\n\")\n",
        "  Sys.sleep(45)\n",
        "}"
      ],
      "metadata": {
        "id": "gWW3tatE12je"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "write_yaml( PARAM, file=\"PARAM.yml\")"
      ],
      "metadata": {
        "id": "B9tB2X4439Hg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "format(Sys.time(), \"%a %b %d %X %Y\")"
      ],
      "metadata": {
        "id": "9zA_W25c15DP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finalmente usted deberá cargar el resultado de su corrida en la Google Sheet Colaborativa,  hoja **TareaHogar-05**\n",
        "<br> Siéntase libre de agregar las columnas que hagan falta a la planilla"
      ],
      "metadata": {
        "id": "UdVZucdLHzZ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seguramente usted realice varias corridas de este script con distintos conjuntos de hiperparámetros, siempre cambiandole el nombre al script  y también cambiando el nombre del experimento,  deberá TODAS esas corridas en distintas lineas de la  Google Sheet Colaborativa, hoja **TareaHogar-05**"
      ],
      "metadata": {
        "id": "5MB_67DmDTh0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Siéntase libre de agregar columnas a la hoja **TareaHogar-05**  en caso de ser necesario."
      ],
      "metadata": {
        "id": "OnRUS_PhFI1Z"
      }
    }
  ]
}